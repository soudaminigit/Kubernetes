* 
* ==> Audit <==
* |-----------|--------------------------------|----------|---------------------------|---------|---------------------|---------------------|
|  Command  |              Args              | Profile  |           User            | Version |     Start Time      |      End Time       |
|-----------|--------------------------------|----------|---------------------------|---------|---------------------|---------------------|
| start     |                                | minikube | DESKTOP-TD00D55\Soudamini | v1.32.0 | 22 Nov 23 17:21 IST |                     |
| start     |                                | minikube | DESKTOP-TD00D55\Soudamini | v1.32.0 | 22 Nov 23 17:22 IST | 22 Nov 23 17:32 IST |
| kubectl   | -- get pods -A                 | minikube | DESKTOP-TD00D55\Soudamini | v1.32.0 | 22 Nov 23 18:22 IST | 22 Nov 23 18:24 IST |
| dashboard |                                | minikube | DESKTOP-TD00D55\Soudamini | v1.32.0 | 22 Nov 23 18:25 IST |                     |
| service   | hello-minikube                 | minikube | DESKTOP-TD00D55\Soudamini | v1.32.0 | 22 Nov 23 18:35 IST | 24 Nov 23 21:41 IST |
| stop      |                                | minikube | DESKTOP-TD00D55\Soudamini | v1.32.0 | 22 Nov 23 18:37 IST | 22 Nov 23 18:37 IST |
| start     |                                | minikube | DESKTOP-TD00D55\Soudamini | v1.32.0 | 27 Nov 23 20:45 IST | 27 Nov 23 20:45 IST |
| service   | hello-node                     | minikube | DESKTOP-TD00D55\Soudamini | v1.32.0 | 27 Nov 23 20:59 IST | 27 Nov 23 21:01 IST |
| start     | -o json -p minikube --user     | minikube | minikube-gui              | v1.32.0 | 27 Nov 23 21:13 IST | 27 Nov 23 21:13 IST |
|           | minikube-gui                   |          |                           |         |                     |                     |
| service   | minikube service list -o json  | minikube | minikube-gui              | v1.32.0 | 27 Nov 23 21:14 IST | 27 Nov 23 21:14 IST |
|           | --user minikube-gui            |          |                           |         |                     |                     |
| start     |                                | minikube | DESKTOP-TD00D55\Soudamini | v1.32.0 | 27 Nov 23 21:22 IST | 27 Nov 23 21:23 IST |
| service   | placement-app                  | minikube | DESKTOP-TD00D55\Soudamini | v1.32.0 | 27 Nov 23 21:28 IST |                     |
| service   | placement-app                  | minikube | DESKTOP-TD00D55\Soudamini | v1.32.0 | 27 Nov 23 21:29 IST |                     |
| start     |                                | minikube | DESKTOP-TD00D55\Soudamini | v1.32.0 | 27 Nov 23 21:30 IST | 27 Nov 23 21:31 IST |
| start     |                                | minikube | DESKTOP-TD00D55\Soudamini | v1.32.0 | 28 Nov 23 19:47 IST | 28 Nov 23 19:47 IST |
| service   | placement-app                  | minikube | DESKTOP-TD00D55\Soudamini | v1.32.0 | 28 Nov 23 19:47 IST |                     |
| service   | placement-app:v1               | minikube | DESKTOP-TD00D55\Soudamini | v1.32.0 | 28 Nov 23 20:07 IST |                     |
| service   | list                           | minikube | DESKTOP-TD00D55\Soudamini | v1.32.0 | 28 Nov 23 20:07 IST | 28 Nov 23 20:07 IST |
| service   | placement-app                  | minikube | DESKTOP-TD00D55\Soudamini | v1.32.0 | 28 Nov 23 20:07 IST |                     |
| service   | placement-app                  | minikube | DESKTOP-TD00D55\Soudamini | v1.32.0 | 28 Nov 23 20:11 IST |                     |
| delete    | placement-app                  | minikube | DESKTOP-TD00D55\Soudamini | v1.32.0 | 28 Nov 23 20:20 IST |                     |
| delete    | deployment placement-app       | minikube | DESKTOP-TD00D55\Soudamini | v1.32.0 | 28 Nov 23 20:20 IST |                     |
| start     |                                | minikube | DESKTOP-TD00D55\Soudamini | v1.32.0 | 28 Nov 23 20:24 IST | 28 Nov 23 20:25 IST |
| service   | placement-app                  | minikube | DESKTOP-TD00D55\Soudamini | v1.32.0 | 28 Nov 23 20:35 IST |                     |
| service   | placement-app                  | minikube | DESKTOP-TD00D55\Soudamini | v1.32.0 | 28 Nov 23 20:45 IST |                     |
| service   | placement-app                  | minikube | DESKTOP-TD00D55\Soudamini | v1.32.0 | 28 Nov 23 20:51 IST |                     |
| service   | placement-app                  | minikube | DESKTOP-TD00D55\Soudamini | v1.32.0 | 28 Nov 23 20:53 IST |                     |
| service   | placement-app                  | minikube | DESKTOP-TD00D55\Soudamini | v1.32.0 | 28 Nov 23 20:57 IST | 28 Nov 23 21:08 IST |
| start     |                                | minikube | DESKTOP-TD00D55\Soudamini | v1.32.0 | 30 Nov 23 21:45 IST | 30 Nov 23 21:46 IST |
| service   | churnmodelling                 | minikube | DESKTOP-TD00D55\Soudamini | v1.32.0 | 30 Nov 23 22:03 IST |                     |
|-----------|--------------------------------|----------|---------------------------|---------|---------------------|---------------------|

* 
* ==> Last Start <==
* Log file created at: 2023/11/30 21:45:40
Running on machine: DESKTOP-TD00D55
Binary: Built with gc go1.21.3 for windows/amd64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I1130 21:45:40.404941   30756 out.go:296] Setting OutFile to fd 96 ...
I1130 21:45:40.406550   30756 out.go:343] TERM=,COLORTERM=, which probably does not support color
I1130 21:45:40.406550   30756 out.go:309] Setting ErrFile to fd 100...
I1130 21:45:40.406550   30756 out.go:343] TERM=,COLORTERM=, which probably does not support color
W1130 21:45:40.435133   30756 root.go:314] Error reading config file at C:\Users\Soudamini\.minikube\config\config.json: open C:\Users\Soudamini\.minikube\config\config.json: The system cannot find the file specified.
I1130 21:45:40.446055   30756 out.go:303] Setting JSON to false
I1130 21:45:40.456139   30756 start.go:128] hostinfo: {"hostname":"DESKTOP-TD00D55","uptime":1396044,"bootTime":1699964895,"procs":391,"os":"windows","platform":"Microsoft Windows 11 Home Single Language","platformFamily":"Standalone Workstation","platformVersion":"10.0.22621.2428 Build 22621.2428","kernelVersion":"10.0.22621.2428 Build 22621.2428","kernelArch":"x86_64","virtualizationSystem":"","virtualizationRole":"","hostId":"bb27143a-4c71-4a7a-a0de-fc288a8e3dac"}
W1130 21:45:40.456139   30756 start.go:136] gopshost.Virtualization returned error: not implemented yet
I1130 21:45:40.457769   30756 out.go:177] * minikube v1.32.0 on Microsoft Windows 11 Home Single Language 10.0.22621.2428 Build 22621.2428
I1130 21:45:40.458861   30756 notify.go:220] Checking for updates...
I1130 21:45:40.459591   30756 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.28.3
I1130 21:45:40.460101   30756 driver.go:378] Setting default libvirt URI to qemu:///system
I1130 21:45:40.727114   30756 docker.go:122] docker version: linux-24.0.6:Docker Desktop 4.25.0 (126437)
I1130 21:45:40.730916   30756 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I1130 21:45:41.242914   30756 info.go:266] docker info: {ID:f07e6fd1-f270-404d-ab3c-93dd7a920e16 Containers:2 ContainersRunning:1 ContainersPaused:0 ContainersStopped:1 Images:3 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:57 OomKillDisable:true NGoroutines:72 SystemTime:2023-11-30 16:15:41.192995518 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:10 KernelVersion:5.15.90.1-microsoft-standard-WSL2 OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:12 MemTotal:7989211136 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:DESKTOP-TD00D55 Labels:[] ExperimentalBuild:false ServerVersion:24.0.6 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:8165feabfdfe38c65b599c4993d227328c231fca Expected:8165feabfdfe38c65b599c4993d227328c231fca} RuncCommit:{ID:v1.1.8-0-g82f18fe Expected:v1.1.8-0-g82f18fe} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=unconfined] ProductLicense: Warnings:[WARNING: No blkio throttle.read_bps_device support WARNING: No blkio throttle.write_bps_device support WARNING: No blkio throttle.read_iops_device support WARNING: No blkio throttle.write_iops_device support WARNING: daemon is not using the default seccomp profile] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:C:\Program Files\Docker\cli-plugins\docker-buildx.exe SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.11.2-desktop.5] map[Name:compose Path:C:\Program Files\Docker\cli-plugins\docker-compose.exe SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.23.0-desktop.1] map[Name:dev Path:C:\Program Files\Docker\cli-plugins\docker-dev.exe SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.0] map[Name:extension Path:C:\Program Files\Docker\cli-plugins\docker-extension.exe SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.20] map[Name:init Path:C:\Program Files\Docker\cli-plugins\docker-init.exe SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v0.1.0-beta.9] map[Name:sbom Path:C:\Program Files\Docker\cli-plugins\docker-sbom.exe SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scan Path:C:\Program Files\Docker\cli-plugins\docker-scan.exe SchemaVersion:0.1.0 ShortDescription:Docker Scan Vendor:Docker Inc. Version:v0.26.0] map[Name:scout Path:C:\Program Files\Docker\cli-plugins\docker-scout.exe SchemaVersion:0.1.0 ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.0.9]] Warnings:<nil>}}
I1130 21:45:41.244541   30756 out.go:177] * Using the docker driver based on existing profile
I1130 21:45:41.245596   30756 start.go:298] selected driver: docker
I1130 21:45:41.245596   30756 start.go:902] validating driver "docker" against &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 Memory:3900 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.28.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.28.3 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[dashboard:true default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\Soudamini:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 AutoPauseInterval:1m0s GPUs:}
I1130 21:45:41.245596   30756 start.go:913] status for docker: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I1130 21:45:41.253590   30756 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I1130 21:45:41.694647   30756 info.go:266] docker info: {ID:f07e6fd1-f270-404d-ab3c-93dd7a920e16 Containers:2 ContainersRunning:1 ContainersPaused:0 ContainersStopped:1 Images:3 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:57 OomKillDisable:true NGoroutines:72 SystemTime:2023-11-30 16:15:41.654323498 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:10 KernelVersion:5.15.90.1-microsoft-standard-WSL2 OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:12 MemTotal:7989211136 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:DESKTOP-TD00D55 Labels:[] ExperimentalBuild:false ServerVersion:24.0.6 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:8165feabfdfe38c65b599c4993d227328c231fca Expected:8165feabfdfe38c65b599c4993d227328c231fca} RuncCommit:{ID:v1.1.8-0-g82f18fe Expected:v1.1.8-0-g82f18fe} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=unconfined] ProductLicense: Warnings:[WARNING: No blkio throttle.read_bps_device support WARNING: No blkio throttle.write_bps_device support WARNING: No blkio throttle.read_iops_device support WARNING: No blkio throttle.write_iops_device support WARNING: daemon is not using the default seccomp profile] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:C:\Program Files\Docker\cli-plugins\docker-buildx.exe SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.11.2-desktop.5] map[Name:compose Path:C:\Program Files\Docker\cli-plugins\docker-compose.exe SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.23.0-desktop.1] map[Name:dev Path:C:\Program Files\Docker\cli-plugins\docker-dev.exe SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.0] map[Name:extension Path:C:\Program Files\Docker\cli-plugins\docker-extension.exe SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.20] map[Name:init Path:C:\Program Files\Docker\cli-plugins\docker-init.exe SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v0.1.0-beta.9] map[Name:sbom Path:C:\Program Files\Docker\cli-plugins\docker-sbom.exe SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scan Path:C:\Program Files\Docker\cli-plugins\docker-scan.exe SchemaVersion:0.1.0 ShortDescription:Docker Scan Vendor:Docker Inc. Version:v0.26.0] map[Name:scout Path:C:\Program Files\Docker\cli-plugins\docker-scout.exe SchemaVersion:0.1.0 ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.0.9]] Warnings:<nil>}}
I1130 21:45:41.819106   30756 cni.go:84] Creating CNI manager for ""
I1130 21:45:41.819106   30756 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I1130 21:45:41.819106   30756 start_flags.go:323] config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 Memory:3900 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.28.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.28.3 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[dashboard:true default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\Soudamini:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 AutoPauseInterval:1m0s GPUs:}
I1130 21:45:41.820284   30756 out.go:177] * Starting control plane node minikube in cluster minikube
I1130 21:45:41.821443   30756 cache.go:121] Beginning downloading kic base image for docker with docker
I1130 21:45:41.822665   30756 out.go:177] * Pulling base image ...
I1130 21:45:41.823190   30756 preload.go:132] Checking if preload exists for k8s version v1.28.3 and runtime docker
I1130 21:45:41.823190   30756 image.go:79] Checking for gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 in local docker daemon
I1130 21:45:41.824832   30756 preload.go:148] Found local preload: C:\Users\Soudamini\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.28.3-docker-overlay2-amd64.tar.lz4
I1130 21:45:41.824832   30756 cache.go:56] Caching tarball of preloaded images
I1130 21:45:41.824832   30756 preload.go:174] Found C:\Users\Soudamini\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.28.3-docker-overlay2-amd64.tar.lz4 in cache, skipping download
I1130 21:45:41.825390   30756 cache.go:59] Finished verifying existence of preloaded tar for  v1.28.3 on docker
I1130 21:45:41.825390   30756 profile.go:148] Saving config to C:\Users\Soudamini\.minikube\profiles\minikube\config.json ...
I1130 21:45:42.025696   30756 image.go:83] Found gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 in local docker daemon, skipping pull
I1130 21:45:42.025696   30756 cache.go:144] gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 exists in daemon, skipping load
I1130 21:45:42.025696   30756 cache.go:194] Successfully downloaded all kic artifacts
I1130 21:45:42.026283   30756 start.go:365] acquiring machines lock for minikube: {Name:mk01ba4ff8e121275722cd9fbe1c521504e79fde Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I1130 21:45:42.026283   30756 start.go:369] acquired machines lock for "minikube" in 0s
I1130 21:45:42.026283   30756 start.go:96] Skipping create...Using existing machine configuration
I1130 21:45:42.026283   30756 fix.go:54] fixHost starting: 
I1130 21:45:42.034588   30756 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1130 21:45:42.253222   30756 fix.go:102] recreateIfNeeded on minikube: state=Running err=<nil>
W1130 21:45:42.253222   30756 fix.go:128] unexpected machine state, will restart: <nil>
I1130 21:45:42.254292   30756 out.go:177] * Updating the running docker "minikube" container ...
I1130 21:45:42.255035   30756 machine.go:88] provisioning docker machine ...
I1130 21:45:42.255035   30756 ubuntu.go:169] provisioning hostname "minikube"
I1130 21:45:42.259855   30756 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1130 21:45:42.475699   30756 main.go:141] libmachine: Using SSH client type: native
I1130 21:45:42.476853   30756 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x14047e0] 0x1407320 <nil>  [] 0s} 127.0.0.1 64380 <nil> <nil>}
I1130 21:45:42.476853   30756 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I1130 21:45:42.871576   30756 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I1130 21:45:42.875554   30756 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1130 21:45:43.130293   30756 main.go:141] libmachine: Using SSH client type: native
I1130 21:45:43.130471   30756 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x14047e0] 0x1407320 <nil>  [] 0s} 127.0.0.1 64380 <nil> <nil>}
I1130 21:45:43.130471   30756 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I1130 21:45:43.281284   30756 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I1130 21:45:43.281822   30756 ubuntu.go:175] set auth options {CertDir:C:\Users\Soudamini\.minikube CaCertPath:C:\Users\Soudamini\.minikube\certs\ca.pem CaPrivateKeyPath:C:\Users\Soudamini\.minikube\certs\ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:C:\Users\Soudamini\.minikube\machines\server.pem ServerKeyPath:C:\Users\Soudamini\.minikube\machines\server-key.pem ClientKeyPath:C:\Users\Soudamini\.minikube\certs\key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:C:\Users\Soudamini\.minikube\certs\cert.pem ServerCertSANs:[] StorePath:C:\Users\Soudamini\.minikube}
I1130 21:45:43.281822   30756 ubuntu.go:177] setting up certificates
I1130 21:45:43.281822   30756 provision.go:83] configureAuth start
I1130 21:45:43.285972   30756 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I1130 21:45:43.488233   30756 provision.go:138] copyHostCerts
I1130 21:45:43.489816   30756 exec_runner.go:144] found C:\Users\Soudamini\.minikube/cert.pem, removing ...
I1130 21:45:43.489816   30756 exec_runner.go:203] rm: C:\Users\Soudamini\.minikube\cert.pem
I1130 21:45:43.490370   30756 exec_runner.go:151] cp: C:\Users\Soudamini\.minikube\certs\cert.pem --> C:\Users\Soudamini\.minikube/cert.pem (1131 bytes)
I1130 21:45:43.491418   30756 exec_runner.go:144] found C:\Users\Soudamini\.minikube/key.pem, removing ...
I1130 21:45:43.491418   30756 exec_runner.go:203] rm: C:\Users\Soudamini\.minikube\key.pem
I1130 21:45:43.491986   30756 exec_runner.go:151] cp: C:\Users\Soudamini\.minikube\certs\key.pem --> C:\Users\Soudamini\.minikube/key.pem (1675 bytes)
I1130 21:45:43.493727   30756 exec_runner.go:144] found C:\Users\Soudamini\.minikube/ca.pem, removing ...
I1130 21:45:43.493727   30756 exec_runner.go:203] rm: C:\Users\Soudamini\.minikube\ca.pem
I1130 21:45:43.493727   30756 exec_runner.go:151] cp: C:\Users\Soudamini\.minikube\certs\ca.pem --> C:\Users\Soudamini\.minikube/ca.pem (1086 bytes)
I1130 21:45:43.494847   30756 provision.go:112] generating server cert: C:\Users\Soudamini\.minikube\machines\server.pem ca-key=C:\Users\Soudamini\.minikube\certs\ca.pem private-key=C:\Users\Soudamini\.minikube\certs\ca-key.pem org=Soudamini.minikube san=[192.168.49.2 127.0.0.1 localhost 127.0.0.1 minikube minikube]
I1130 21:45:43.619833   30756 provision.go:172] copyRemoteCerts
I1130 21:45:43.625229   30756 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I1130 21:45:43.628645   30756 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1130 21:45:43.831805   30756 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:64380 SSHKeyPath:C:\Users\Soudamini\.minikube\machines\minikube\id_rsa Username:docker}
I1130 21:45:43.946683   30756 ssh_runner.go:362] scp C:\Users\Soudamini\.minikube\certs\ca.pem --> /etc/docker/ca.pem (1086 bytes)
I1130 21:45:44.014678   30756 ssh_runner.go:362] scp C:\Users\Soudamini\.minikube\machines\server.pem --> /etc/docker/server.pem (1208 bytes)
I1130 21:45:44.062942   30756 ssh_runner.go:362] scp C:\Users\Soudamini\.minikube\machines\server-key.pem --> /etc/docker/server-key.pem (1679 bytes)
I1130 21:45:44.112949   30756 provision.go:86] duration metric: configureAuth took 831.1272ms
I1130 21:45:44.113461   30756 ubuntu.go:193] setting minikube options for container-runtime
I1130 21:45:44.114010   30756 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.28.3
I1130 21:45:44.118002   30756 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1130 21:45:44.303766   30756 main.go:141] libmachine: Using SSH client type: native
I1130 21:45:44.304366   30756 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x14047e0] 0x1407320 <nil>  [] 0s} 127.0.0.1 64380 <nil> <nil>}
I1130 21:45:44.304366   30756 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I1130 21:45:44.465128   30756 main.go:141] libmachine: SSH cmd err, output: <nil>: overlay

I1130 21:45:44.465671   30756 ubuntu.go:71] root file system type: overlay
I1130 21:45:44.465671   30756 provision.go:309] Updating docker unit: /lib/systemd/system/docker.service ...
I1130 21:45:44.470043   30756 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1130 21:45:44.664803   30756 main.go:141] libmachine: Using SSH client type: native
I1130 21:45:44.665388   30756 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x14047e0] 0x1407320 <nil>  [] 0s} 127.0.0.1 64380 <nil> <nil>}
I1130 21:45:44.665388   30756 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %!s(MISSING) "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I1130 21:45:44.841851   30756 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I1130 21:45:44.845722   30756 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1130 21:45:45.054985   30756 main.go:141] libmachine: Using SSH client type: native
I1130 21:45:45.055642   30756 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x14047e0] 0x1407320 <nil>  [] 0s} 127.0.0.1 64380 <nil> <nil>}
I1130 21:45:45.055642   30756 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I1130 21:45:45.212951   30756 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I1130 21:45:45.212951   30756 machine.go:91] provisioned docker machine in 2.9579151s
I1130 21:45:45.212951   30756 start.go:300] post-start starting for "minikube" (driver="docker")
I1130 21:45:45.212951   30756 start.go:329] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I1130 21:45:45.219391   30756 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I1130 21:45:45.222718   30756 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1130 21:45:45.422683   30756 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:64380 SSHKeyPath:C:\Users\Soudamini\.minikube\machines\minikube\id_rsa Username:docker}
I1130 21:45:45.534431   30756 ssh_runner.go:195] Run: cat /etc/os-release
I1130 21:45:45.541568   30756 main.go:141] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I1130 21:45:45.541568   30756 main.go:141] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I1130 21:45:45.541568   30756 main.go:141] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I1130 21:45:45.541568   30756 info.go:137] Remote host: Ubuntu 22.04.3 LTS
I1130 21:45:45.541568   30756 filesync.go:126] Scanning C:\Users\Soudamini\.minikube\addons for local assets ...
I1130 21:45:45.542112   30756 filesync.go:126] Scanning C:\Users\Soudamini\.minikube\files for local assets ...
I1130 21:45:45.542112   30756 start.go:303] post-start completed in 329.1611ms
I1130 21:45:45.547693   30756 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I1130 21:45:45.552372   30756 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1130 21:45:45.760732   30756 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:64380 SSHKeyPath:C:\Users\Soudamini\.minikube\machines\minikube\id_rsa Username:docker}
I1130 21:45:45.870622   30756 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I1130 21:45:45.877163   30756 fix.go:56] fixHost completed within 3.8508804s
I1130 21:45:45.877163   30756 start.go:83] releasing machines lock for "minikube", held for 3.8508804s
I1130 21:45:45.881017   30756 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I1130 21:45:46.075229   30756 ssh_runner.go:195] Run: curl -sS -m 2 https://registry.k8s.io/
I1130 21:45:46.079639   30756 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1130 21:45:46.082360   30756 ssh_runner.go:195] Run: cat /version.json
I1130 21:45:46.085630   30756 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1130 21:45:46.298201   30756 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:64380 SSHKeyPath:C:\Users\Soudamini\.minikube\machines\minikube\id_rsa Username:docker}
I1130 21:45:46.311947   30756 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:64380 SSHKeyPath:C:\Users\Soudamini\.minikube\machines\minikube\id_rsa Username:docker}
I1130 21:45:46.401425   30756 ssh_runner.go:195] Run: systemctl --version
I1130 21:45:47.215498   30756 ssh_runner.go:235] Completed: curl -sS -m 2 https://registry.k8s.io/: (1.1402691s)
I1130 21:45:47.222619   30756 ssh_runner.go:195] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
I1130 21:45:47.238367   30756 ssh_runner.go:195] Run: sudo find \etc\cni\net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;
W1130 21:45:47.265500   30756 start.go:416] unable to name loopback interface in configureRuntimes: unable to patch loopback cni config "/etc/cni/net.d/*loopback.conf*": sudo find \etc\cni\net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;: Process exited with status 1
stdout:

stderr:
find: '\\etc\\cni\\net.d': No such file or directory
I1130 21:45:47.271521   30756 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%!p(MISSING), " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I1130 21:45:47.289014   30756 cni.go:259] no active bridge cni configs found in "/etc/cni/net.d" - nothing to disable
I1130 21:45:47.289014   30756 start.go:472] detecting cgroup driver to use...
I1130 21:45:47.289014   30756 detect.go:196] detected "cgroupfs" cgroup driver on host os
I1130 21:45:47.291508   30756 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I1130 21:45:47.330756   30756 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.9"|' /etc/containerd/config.toml"
I1130 21:45:47.370488   30756 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I1130 21:45:47.393428   30756 containerd.go:145] configuring containerd to use "cgroupfs" as cgroup driver...
I1130 21:45:47.400031   30756 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = false|g' /etc/containerd/config.toml"
I1130 21:45:47.429304   30756 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I1130 21:45:47.457423   30756 ssh_runner.go:195] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I1130 21:45:47.489217   30756 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I1130 21:45:47.516852   30756 ssh_runner.go:195] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I1130 21:45:47.543792   30756 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I1130 21:45:47.573062   30756 ssh_runner.go:195] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I1130 21:45:47.603688   30756 ssh_runner.go:195] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I1130 21:45:47.632599   30756 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1130 21:45:47.981035   30756 ssh_runner.go:195] Run: sudo systemctl restart containerd
I1130 21:45:59.922905   30756 ssh_runner.go:235] Completed: sudo systemctl restart containerd: (11.9418702s)
I1130 21:45:59.922905   30756 start.go:472] detecting cgroup driver to use...
I1130 21:45:59.922905   30756 detect.go:196] detected "cgroupfs" cgroup driver on host os
I1130 21:45:59.932942   30756 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I1130 21:45:59.959775   30756 cruntime.go:279] skipping containerd shutdown because we are bound to it
I1130 21:45:59.966353   30756 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I1130 21:45:59.991059   30756 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I1130 21:46:00.045539   30756 ssh_runner.go:195] Run: which cri-dockerd
I1130 21:46:00.060012   30756 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I1130 21:46:00.079604   30756 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (189 bytes)
I1130 21:46:00.124222   30756 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I1130 21:46:00.307238   30756 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I1130 21:46:00.533586   30756 docker.go:560] configuring docker to use "cgroupfs" as cgroup driver...
I1130 21:46:00.534103   30756 ssh_runner.go:362] scp memory --> /etc/docker/daemon.json (130 bytes)
I1130 21:46:00.578250   30756 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1130 21:46:00.814030   30756 ssh_runner.go:195] Run: sudo systemctl restart docker
I1130 21:46:01.567243   30756 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I1130 21:46:01.747281   30756 ssh_runner.go:195] Run: sudo systemctl unmask cri-docker.socket
I1130 21:46:01.889915   30756 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I1130 21:46:02.032949   30756 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1130 21:46:02.185214   30756 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.socket
I1130 21:46:02.239179   30756 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1130 21:46:02.407162   30756 ssh_runner.go:195] Run: sudo systemctl restart cri-docker
I1130 21:46:02.645837   30756 start.go:519] Will wait 60s for socket path /var/run/cri-dockerd.sock
I1130 21:46:02.653190   30756 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I1130 21:46:02.663252   30756 start.go:540] Will wait 60s for crictl version
I1130 21:46:02.671199   30756 ssh_runner.go:195] Run: which crictl
I1130 21:46:02.687877   30756 ssh_runner.go:195] Run: sudo /usr/bin/crictl version
I1130 21:46:02.956112   30756 start.go:556] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  24.0.7
RuntimeApiVersion:  v1
I1130 21:46:02.960709   30756 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I1130 21:46:03.128271   30756 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I1130 21:46:03.169537   30756 out.go:204] * Preparing Kubernetes v1.28.3 on Docker 24.0.7 ...
I1130 21:46:03.174116   30756 cli_runner.go:164] Run: docker exec -t minikube dig +short host.docker.internal
I1130 21:46:03.732870   30756 network.go:96] got host ip for mount in container by digging dns: 192.168.65.254
I1130 21:46:03.739776   30756 ssh_runner.go:195] Run: grep 192.168.65.254	host.minikube.internal$ /etc/hosts
I1130 21:46:03.750116   30756 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I1130 21:46:03.969440   30756 preload.go:132] Checking if preload exists for k8s version v1.28.3 and runtime docker
I1130 21:46:03.973885   30756 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I1130 21:46:04.006853   30756 docker.go:671] Got preloaded images: -- stdout --
registry.k8s.io/kube-apiserver:v1.28.3
registry.k8s.io/kube-controller-manager:v1.28.3
registry.k8s.io/kube-scheduler:v1.28.3
registry.k8s.io/kube-proxy:v1.28.3
registry.k8s.io/etcd:3.5.9-0
registry.k8s.io/coredns/coredns:v1.10.1
registry.k8s.io/pause:3.9
kubernetesui/dashboard:<none>
kicbase/echo-server:1.0
kubernetesui/metrics-scraper:<none>
<none>:<none>
subbu0319/placement-app:latest
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I1130 21:46:04.006853   30756 docker.go:601] Images already preloaded, skipping extraction
I1130 21:46:04.010652   30756 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I1130 21:46:04.038876   30756 docker.go:671] Got preloaded images: -- stdout --
registry.k8s.io/kube-apiserver:v1.28.3
registry.k8s.io/kube-scheduler:v1.28.3
registry.k8s.io/kube-controller-manager:v1.28.3
registry.k8s.io/kube-proxy:v1.28.3
registry.k8s.io/etcd:3.5.9-0
registry.k8s.io/coredns/coredns:v1.10.1
registry.k8s.io/pause:3.9
kubernetesui/dashboard:<none>
kicbase/echo-server:1.0
kubernetesui/metrics-scraper:<none>
<none>:<none>
subbu0319/placement-app:latest
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I1130 21:46:04.038876   30756 cache_images.go:84] Images are preloaded, skipping loading
I1130 21:46:04.042732   30756 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I1130 21:46:04.292451   30756 cni.go:84] Creating CNI manager for ""
I1130 21:46:04.292451   30756 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I1130 21:46:04.292451   30756 kubeadm.go:87] Using pod CIDR: 10.244.0.0/16
I1130 21:46:04.292451   30756 kubeadm.go:176] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.49.2 APIServerPort:8443 KubernetesVersion:v1.28.3 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.49.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.49.2 CgroupDriver:cgroupfs ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false KubeletConfigOpts:map[hairpinMode:hairpin-veth runtimeRequestTimeout:15m] PrependCriSocketUnix:true}
I1130 21:46:04.292996   30756 kubeadm.go:181] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta3
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.49.2
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: unix:///var/run/cri-dockerd.sock
  name: "minikube"
  kubeletExtraArgs:
    node-ip: 192.168.49.2
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta3
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.49.2"]
  extraArgs:
    enable-admission-plugins: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    allocate-node-cidrs: "true"
    leader-elect: "false"
scheduler:
  extraArgs:
    leader-elect: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      proxy-refresh-interval: "70000"
kubernetesVersion: v1.28.3
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: cgroupfs
hairpinMode: hairpin-veth
runtimeRequestTimeout: 15m
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%!"(MISSING)
  nodefs.inodesFree: "0%!"(MISSING)
  imagefs.available: "0%!"(MISSING)
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I1130 21:46:04.292996   30756 kubeadm.go:976] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.28.3/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --container-runtime-endpoint=unix:///var/run/cri-dockerd.sock --hostname-override=minikube --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.49.2

[Install]
 config:
{KubernetesVersion:v1.28.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:}
I1130 21:46:04.300666   30756 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.28.3
I1130 21:46:04.326485   30756 binaries.go:44] Found k8s binaries, skipping transfer
I1130 21:46:04.332527   30756 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I1130 21:46:04.350783   30756 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (369 bytes)
I1130 21:46:04.387919   30756 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I1130 21:46:04.422353   30756 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2091 bytes)
I1130 21:46:04.464001   30756 ssh_runner.go:195] Run: grep 192.168.49.2	control-plane.minikube.internal$ /etc/hosts
I1130 21:46:04.470109   30756 certs.go:56] Setting up C:\Users\Soudamini\.minikube\profiles\minikube for IP: 192.168.49.2
I1130 21:46:04.470638   30756 certs.go:190] acquiring lock for shared ca certs: {Name:mkd722a8510f42ab87626082bc764cf6887c8e0a Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1130 21:46:04.471179   30756 certs.go:199] skipping minikubeCA CA generation: C:\Users\Soudamini\.minikube\ca.key
I1130 21:46:04.471712   30756 certs.go:199] skipping proxyClientCA CA generation: C:\Users\Soudamini\.minikube\proxy-client-ca.key
I1130 21:46:04.472251   30756 certs.go:315] skipping minikube-user signed cert generation: C:\Users\Soudamini\.minikube\profiles\minikube\client.key
I1130 21:46:04.473405   30756 certs.go:315] skipping minikube signed cert generation: C:\Users\Soudamini\.minikube\profiles\minikube\apiserver.key.dd3b5fb2
I1130 21:46:04.475000   30756 certs.go:315] skipping aggregator signed cert generation: C:\Users\Soudamini\.minikube\profiles\minikube\proxy-client.key
I1130 21:46:04.476150   30756 certs.go:437] found cert: C:\Users\Soudamini\.minikube\certs\C:\Users\Soudamini\.minikube\certs\ca-key.pem (1679 bytes)
I1130 21:46:04.476683   30756 certs.go:437] found cert: C:\Users\Soudamini\.minikube\certs\C:\Users\Soudamini\.minikube\certs\ca.pem (1086 bytes)
I1130 21:46:04.476683   30756 certs.go:437] found cert: C:\Users\Soudamini\.minikube\certs\C:\Users\Soudamini\.minikube\certs\cert.pem (1131 bytes)
I1130 21:46:04.476683   30756 certs.go:437] found cert: C:\Users\Soudamini\.minikube\certs\C:\Users\Soudamini\.minikube\certs\key.pem (1675 bytes)
I1130 21:46:04.477813   30756 ssh_runner.go:362] scp C:\Users\Soudamini\.minikube\profiles\minikube\apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1399 bytes)
I1130 21:46:04.530496   30756 ssh_runner.go:362] scp C:\Users\Soudamini\.minikube\profiles\minikube\apiserver.key --> /var/lib/minikube/certs/apiserver.key (1679 bytes)
I1130 21:46:04.581673   30756 ssh_runner.go:362] scp C:\Users\Soudamini\.minikube\profiles\minikube\proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I1130 21:46:04.629596   30756 ssh_runner.go:362] scp C:\Users\Soudamini\.minikube\profiles\minikube\proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1679 bytes)
I1130 21:46:04.678302   30756 ssh_runner.go:362] scp C:\Users\Soudamini\.minikube\ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I1130 21:46:04.736195   30756 ssh_runner.go:362] scp C:\Users\Soudamini\.minikube\ca.key --> /var/lib/minikube/certs/ca.key (1675 bytes)
I1130 21:46:04.784882   30756 ssh_runner.go:362] scp C:\Users\Soudamini\.minikube\proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I1130 21:46:04.835465   30756 ssh_runner.go:362] scp C:\Users\Soudamini\.minikube\proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1679 bytes)
I1130 21:46:04.881455   30756 ssh_runner.go:362] scp C:\Users\Soudamini\.minikube\ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I1130 21:46:04.928946   30756 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (738 bytes)
I1130 21:46:04.972356   30756 ssh_runner.go:195] Run: openssl version
I1130 21:46:04.999040   30756 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I1130 21:46:05.028357   30756 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I1130 21:46:05.035784   30756 certs.go:480] hashing: -rw-r--r-- 1 root root 1111 Nov 22 12:01 /usr/share/ca-certificates/minikubeCA.pem
I1130 21:46:05.041433   30756 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I1130 21:46:05.058176   30756 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I1130 21:46:05.083670   30756 ssh_runner.go:195] Run: ls /var/lib/minikube/certs/etcd
I1130 21:46:05.095109   30756 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-etcd-client.crt -checkend 86400
I1130 21:46:05.110609   30756 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-kubelet-client.crt -checkend 86400
I1130 21:46:05.126058   30756 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/server.crt -checkend 86400
I1130 21:46:05.141520   30756 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/healthcheck-client.crt -checkend 86400
I1130 21:46:05.157116   30756 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/peer.crt -checkend 86400
I1130 21:46:05.173671   30756 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/front-proxy-client.crt -checkend 86400
I1130 21:46:05.184058   30756 kubeadm.go:404] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 Memory:3900 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.28.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.28.3 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[dashboard:true default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\Soudamini:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 AutoPauseInterval:1m0s GPUs:}
I1130 21:46:05.187857   30756 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I1130 21:46:05.221804   30756 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
I1130 21:46:05.239832   30756 kubeadm.go:419] found existing configuration files, will attempt cluster restart
I1130 21:46:05.239832   30756 kubeadm.go:636] restartCluster start
I1130 21:46:05.245260   30756 ssh_runner.go:195] Run: sudo test -d /data/minikube
I1130 21:46:05.264176   30756 kubeadm.go:127] /data/minikube skipping compat symlinks: sudo test -d /data/minikube: Process exited with status 1
stdout:

stderr:
I1130 21:46:05.268406   30756 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I1130 21:46:05.467508   30756 kubeconfig.go:92] found "minikube" server: "https://127.0.0.1:64384"
I1130 21:46:05.475775   30756 ssh_runner.go:195] Run: sudo diff -u /var/tmp/minikube/kubeadm.yaml /var/tmp/minikube/kubeadm.yaml.new
I1130 21:46:05.496835   30756 api_server.go:166] Checking apiserver status ...
I1130 21:46:05.503927   30756 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1130 21:46:05.527669   30756 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1130 21:46:05.527669   30756 api_server.go:166] Checking apiserver status ...
I1130 21:46:05.536182   30756 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1130 21:46:05.555849   30756 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1130 21:46:06.069634   30756 api_server.go:166] Checking apiserver status ...
I1130 21:46:06.075636   30756 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1130 21:46:06.107731   30756 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1130 21:46:06.556578   30756 api_server.go:166] Checking apiserver status ...
I1130 21:46:06.564567   30756 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1130 21:46:06.787102   30756 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1130 21:46:07.063543   30756 api_server.go:166] Checking apiserver status ...
I1130 21:46:07.069552   30756 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1130 21:46:07.296631   30756 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1130 21:46:07.564065   30756 api_server.go:166] Checking apiserver status ...
I1130 21:46:07.571813   30756 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1130 21:46:07.905490   30756 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1130 21:46:08.062718   30756 api_server.go:166] Checking apiserver status ...
I1130 21:46:08.071362   30756 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1130 21:46:08.310765   30756 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1130 21:46:08.563302   30756 api_server.go:166] Checking apiserver status ...
I1130 21:46:08.569921   30756 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1130 21:46:08.795014   30756 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1130 21:46:09.070041   30756 api_server.go:166] Checking apiserver status ...
I1130 21:46:09.076967   30756 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1130 21:46:09.290998   30756 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1130 21:46:09.567176   30756 api_server.go:166] Checking apiserver status ...
I1130 21:46:09.574422   30756 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1130 21:46:09.708789   30756 ssh_runner.go:195] Run: sudo egrep ^[0-9]+:freezer: /proc/634402/cgroup
I1130 21:46:10.105283   30756 api_server.go:182] apiserver freezer: "7:freezer:/docker/c3b8ff854cc296508533d83fe41262d62b30a8532e5c5a4bc48a6eafae71b939/kubepods/burstable/pod55b4bbe24dac3803a7379f9ae169d6ba/8d77bf2b8840bbd8a5084dcf27e947b3073cd38184897db15d0a9c6607e67ead"
I1130 21:46:10.111960   30756 ssh_runner.go:195] Run: sudo cat /sys/fs/cgroup/freezer/docker/c3b8ff854cc296508533d83fe41262d62b30a8532e5c5a4bc48a6eafae71b939/kubepods/burstable/pod55b4bbe24dac3803a7379f9ae169d6ba/8d77bf2b8840bbd8a5084dcf27e947b3073cd38184897db15d0a9c6607e67ead/freezer.state
I1130 21:46:10.396052   30756 api_server.go:204] freezer state: "THAWED"
I1130 21:46:10.396052   30756 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:64384/healthz ...
I1130 21:46:10.413486   30756 api_server.go:269] stopped: https://127.0.0.1:64384/healthz: Get "https://127.0.0.1:64384/healthz": EOF
I1130 21:46:10.413486   30756 retry.go:31] will retry after 231.55497ms: state is "Stopped"
I1130 21:46:10.650866   30756 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:64384/healthz ...
I1130 21:46:10.654575   30756 api_server.go:269] stopped: https://127.0.0.1:64384/healthz: Get "https://127.0.0.1:64384/healthz": EOF
I1130 21:46:10.654575   30756 retry.go:31] will retry after 325.348122ms: state is "Stopped"
I1130 21:46:10.989259   30756 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:64384/healthz ...
I1130 21:46:10.992405   30756 api_server.go:269] stopped: https://127.0.0.1:64384/healthz: Get "https://127.0.0.1:64384/healthz": EOF
I1130 21:46:10.992918   30756 retry.go:31] will retry after 459.616203ms: state is "Stopped"
I1130 21:46:11.460639   30756 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:64384/healthz ...
I1130 21:46:11.464360   30756 api_server.go:269] stopped: https://127.0.0.1:64384/healthz: Get "https://127.0.0.1:64384/healthz": EOF
I1130 21:46:11.464360   30756 retry.go:31] will retry after 588.613131ms: state is "Stopped"
I1130 21:46:12.059842   30756 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:64384/healthz ...
I1130 21:46:12.063410   30756 api_server.go:269] stopped: https://127.0.0.1:64384/healthz: Get "https://127.0.0.1:64384/healthz": EOF
I1130 21:46:12.063937   30756 retry.go:31] will retry after 670.861878ms: state is "Stopped"
I1130 21:46:12.736035   30756 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:64384/healthz ...
I1130 21:46:12.739844   30756 api_server.go:269] stopped: https://127.0.0.1:64384/healthz: Get "https://127.0.0.1:64384/healthz": EOF
I1130 21:46:12.739844   30756 retry.go:31] will retry after 937.274719ms: state is "Stopped"
I1130 21:46:13.677480   30756 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:64384/healthz ...
I1130 21:46:13.681199   30756 api_server.go:269] stopped: https://127.0.0.1:64384/healthz: Get "https://127.0.0.1:64384/healthz": EOF
I1130 21:46:13.681755   30756 retry.go:31] will retry after 1.001824612s: state is "Stopped"
I1130 21:46:14.690869   30756 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:64384/healthz ...
I1130 21:46:14.695616   30756 api_server.go:269] stopped: https://127.0.0.1:64384/healthz: Get "https://127.0.0.1:64384/healthz": EOF
I1130 21:46:14.695616   30756 retry.go:31] will retry after 982.094389ms: state is "Stopped"
I1130 21:46:15.683255   30756 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:64384/healthz ...
I1130 21:46:15.688277   30756 api_server.go:269] stopped: https://127.0.0.1:64384/healthz: Get "https://127.0.0.1:64384/healthz": EOF
I1130 21:46:15.688277   30756 retry.go:31] will retry after 1.791185719s: state is "Stopped"
I1130 21:46:17.493109   30756 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:64384/healthz ...
I1130 21:46:21.307927   30756 api_server.go:279] https://127.0.0.1:64384/healthz returned 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
I1130 21:46:21.307927   30756 retry.go:31] will retry after 2.067844814s: https://127.0.0.1:64384/healthz returned error 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
I1130 21:46:23.378410   30756 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:64384/healthz ...
I1130 21:46:23.391552   30756 api_server.go:279] https://127.0.0.1:64384/healthz returned 200:
ok
I1130 21:46:23.428511   30756 system_pods.go:86] 7 kube-system pods found
I1130 21:46:23.428511   30756 system_pods.go:89] "coredns-5dd5756b68-mkpn4" [49002ca4-37e9-4f8d-a1ad-07297f94a5fa] Running / Ready:ContainersNotReady (containers with unready status: [coredns]) / ContainersReady:ContainersNotReady (containers with unready status: [coredns])
I1130 21:46:23.428511   30756 system_pods.go:89] "etcd-minikube" [0e92e4ef-6653-4bc0-ac14-b6f2814922e2] Running / Ready:ContainersNotReady (containers with unready status: [etcd]) / ContainersReady:ContainersNotReady (containers with unready status: [etcd])
I1130 21:46:23.428511   30756 system_pods.go:89] "kube-apiserver-minikube" [cc1ead11-979a-4af8-97c9-1af57e7e710b] Running / Ready:ContainersNotReady (containers with unready status: [kube-apiserver]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-apiserver])
I1130 21:46:23.428511   30756 system_pods.go:89] "kube-controller-manager-minikube" [624c5c40-db33-421c-8d01-ad55d9b7ed99] Running / Ready:ContainersNotReady (containers with unready status: [kube-controller-manager]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-controller-manager])
I1130 21:46:23.428511   30756 system_pods.go:89] "kube-proxy-nhk66" [5fe9904a-ab92-4eb4-a075-dd97140d5780] Running
I1130 21:46:23.428511   30756 system_pods.go:89] "kube-scheduler-minikube" [74f21923-d400-4fbc-8030-510657ccc803] Running / Ready:ContainersNotReady (containers with unready status: [kube-scheduler]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-scheduler])
I1130 21:46:23.428511   30756 system_pods.go:89] "storage-provisioner" [9d33a503-5e3a-4089-ac3f-9a057e228d8f] Running / Ready:ContainersNotReady (containers with unready status: [storage-provisioner]) / ContainersReady:ContainersNotReady (containers with unready status: [storage-provisioner])
I1130 21:46:23.431493   30756 api_server.go:141] control plane version: v1.28.3
I1130 21:46:23.431493   30756 kubeadm.go:630] The running cluster does not require reconfiguration: 127.0.0.1
I1130 21:46:23.431493   30756 kubeadm.go:684] Taking a shortcut, as the cluster seems to be properly configured
I1130 21:46:23.432056   30756 kubeadm.go:640] restartCluster took 18.1922237s
I1130 21:46:23.432056   30756 kubeadm.go:406] StartCluster complete in 18.2479975s
I1130 21:46:23.432056   30756 settings.go:142] acquiring lock: {Name:mk27224d0426f859fda347f66bbe7ffca74a9130 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1130 21:46:23.432056   30756 settings.go:150] Updating kubeconfig:  C:\Users\Soudamini\.kube\config
I1130 21:46:23.433827   30756 lock.go:35] WriteFile acquiring C:\Users\Soudamini\.kube\config: {Name:mk4ffa09b2025bee546f5eb678dfc37042d89f79 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1130 21:46:23.434396   30756 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml"
I1130 21:46:23.434396   30756 addons.go:499] enable addons start: toEnable=map[ambassador:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:true default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false helm-tiller:false inaccel:false ingress:false ingress-dns:false inspektor-gadget:false istio:false istio-provisioner:false kong:false kubeflow:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-device-plugin:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false storage-provisioner-rancher:false volumesnapshots:false]
I1130 21:46:23.434396   30756 addons.go:69] Setting storage-provisioner=true in profile "minikube"
I1130 21:46:23.434396   30756 addons.go:69] Setting dashboard=true in profile "minikube"
I1130 21:46:23.434396   30756 addons.go:69] Setting default-storageclass=true in profile "minikube"
I1130 21:46:23.434396   30756 addons.go:231] Setting addon storage-provisioner=true in "minikube"
I1130 21:46:23.434396   30756 addons.go:231] Setting addon dashboard=true in "minikube"
W1130 21:46:23.434396   30756 addons.go:240] addon dashboard should already be in state true
W1130 21:46:23.434396   30756 addons.go:240] addon storage-provisioner should already be in state true
I1130 21:46:23.434396   30756 addons_storage_classes.go:33] enableOrDisableStorageClasses default-storageclass=true on "minikube"
I1130 21:46:23.434960   30756 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.28.3
I1130 21:46:23.435532   30756 host.go:66] Checking if "minikube" exists ...
I1130 21:46:23.435532   30756 host.go:66] Checking if "minikube" exists ...
I1130 21:46:23.446662   30756 kapi.go:248] "coredns" deployment in "kube-system" namespace and "minikube" context rescaled to 1 replicas
I1130 21:46:23.446662   30756 start.go:223] Will wait 6m0s for node &{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.28.3 ContainerRuntime:docker ControlPlane:true Worker:true}
I1130 21:46:23.447781   30756 out.go:177] * Verifying Kubernetes components...
I1130 21:46:23.456530   30756 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1130 21:46:23.458651   30756 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1130 21:46:23.461685   30756 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1130 21:46:23.465593   30756 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service kubelet
I1130 21:46:23.756379   30756 out.go:177]   - Using image docker.io/kubernetesui/dashboard:v2.7.0
I1130 21:46:23.758041   30756 out.go:177]   - Using image docker.io/kubernetesui/metrics-scraper:v1.0.8
I1130 21:46:23.758584   30756 addons.go:423] installing /etc/kubernetes/addons/dashboard-ns.yaml
I1130 21:46:23.758584   30756 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-ns.yaml (759 bytes)
I1130 21:46:23.764172   30756 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1130 21:46:23.768368   30756 out.go:177]   - Using image gcr.io/k8s-minikube/storage-provisioner:v5
I1130 21:46:23.771216   30756 addons.go:423] installing /etc/kubernetes/addons/storage-provisioner.yaml
I1130 21:46:23.771216   30756 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2676 bytes)
I1130 21:46:23.784882   30756 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1130 21:46:23.794854   30756 addons.go:231] Setting addon default-storageclass=true in "minikube"
W1130 21:46:23.794854   30756 addons.go:240] addon default-storageclass should already be in state true
I1130 21:46:23.795438   30756 host.go:66] Checking if "minikube" exists ...
I1130 21:46:23.811725   30756 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1130 21:46:24.037768   30756 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:64380 SSHKeyPath:C:\Users\Soudamini\.minikube\machines\minikube\id_rsa Username:docker}
I1130 21:46:24.066068   30756 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:64380 SSHKeyPath:C:\Users\Soudamini\.minikube\machines\minikube\id_rsa Username:docker}
I1130 21:46:24.081566   30756 addons.go:423] installing /etc/kubernetes/addons/storageclass.yaml
I1130 21:46:24.081566   30756 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)
I1130 21:46:24.087791   30756 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1130 21:46:24.107203   30756 start.go:899] CoreDNS already contains "host.minikube.internal" host record, skipping...
I1130 21:46:24.111723   30756 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I1130 21:46:24.181914   30756 addons.go:423] installing /etc/kubernetes/addons/dashboard-clusterrole.yaml
I1130 21:46:24.181914   30756 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-clusterrole.yaml (1001 bytes)
I1130 21:46:24.211905   30756 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.28.3/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml
I1130 21:46:24.221767   30756 addons.go:423] installing /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml
I1130 21:46:24.221767   30756 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml (1018 bytes)
I1130 21:46:24.264103   30756 addons.go:423] installing /etc/kubernetes/addons/dashboard-configmap.yaml
I1130 21:46:24.264103   30756 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-configmap.yaml (837 bytes)
I1130 21:46:24.314781   30756 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:64380 SSHKeyPath:C:\Users\Soudamini\.minikube\machines\minikube\id_rsa Username:docker}
I1130 21:46:24.314781   30756 addons.go:423] installing /etc/kubernetes/addons/dashboard-dp.yaml
I1130 21:46:24.314781   30756 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-dp.yaml (4288 bytes)
I1130 21:46:24.347365   30756 api_server.go:52] waiting for apiserver process to appear ...
I1130 21:46:24.355217   30756 addons.go:423] installing /etc/kubernetes/addons/dashboard-role.yaml
I1130 21:46:24.355217   30756 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-role.yaml (1724 bytes)
I1130 21:46:24.355760   30756 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1130 21:46:24.414707   30756 addons.go:423] installing /etc/kubernetes/addons/dashboard-rolebinding.yaml
I1130 21:46:24.414707   30756 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-rolebinding.yaml (1046 bytes)
I1130 21:46:24.457545   30756 addons.go:423] installing /etc/kubernetes/addons/dashboard-sa.yaml
I1130 21:46:24.457545   30756 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-sa.yaml (837 bytes)
I1130 21:46:24.459726   30756 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.28.3/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml
I1130 21:46:24.519166   30756 addons.go:423] installing /etc/kubernetes/addons/dashboard-secret.yaml
I1130 21:46:24.519689   30756 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-secret.yaml (1389 bytes)
I1130 21:46:24.624057   30756 addons.go:423] installing /etc/kubernetes/addons/dashboard-svc.yaml
I1130 21:46:24.624057   30756 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-svc.yaml (1294 bytes)
I1130 21:46:24.716721   30756 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.28.3/kubectl apply -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml
I1130 21:46:26.028779   30756 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.28.3/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: (1.8168741s)
I1130 21:46:26.028779   30756 ssh_runner.go:235] Completed: sudo pgrep -xnf kube-apiserver.*minikube.*: (1.6730189s)
I1130 21:46:26.029321   30756 api_server.go:72] duration metric: took 2.5821168s to wait for apiserver process to appear ...
I1130 21:46:26.029321   30756 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.28.3/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: (1.5695954s)
I1130 21:46:26.029321   30756 api_server.go:88] waiting for apiserver healthz status ...
I1130 21:46:26.029321   30756 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:64384/healthz ...
I1130 21:46:26.041410   30756 api_server.go:279] https://127.0.0.1:64384/healthz returned 200:
ok
I1130 21:46:26.084857   30756 api_server.go:141] control plane version: v1.28.3
I1130 21:46:26.084857   30756 api_server.go:131] duration metric: took 55.536ms to wait for apiserver health ...
I1130 21:46:26.084857   30756 system_pods.go:43] waiting for kube-system pods to appear ...
I1130 21:46:26.094730   30756 system_pods.go:59] 7 kube-system pods found
I1130 21:46:26.094730   30756 system_pods.go:61] "coredns-5dd5756b68-mkpn4" [49002ca4-37e9-4f8d-a1ad-07297f94a5fa] Running / Ready:ContainersNotReady (containers with unready status: [coredns]) / ContainersReady:ContainersNotReady (containers with unready status: [coredns])
I1130 21:46:26.094730   30756 system_pods.go:61] "etcd-minikube" [0e92e4ef-6653-4bc0-ac14-b6f2814922e2] Running
I1130 21:46:26.094730   30756 system_pods.go:61] "kube-apiserver-minikube" [cc1ead11-979a-4af8-97c9-1af57e7e710b] Running / Ready:ContainersNotReady (containers with unready status: [kube-apiserver]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-apiserver])
I1130 21:46:26.094730   30756 system_pods.go:61] "kube-controller-manager-minikube" [624c5c40-db33-421c-8d01-ad55d9b7ed99] Running
I1130 21:46:26.094730   30756 system_pods.go:61] "kube-proxy-nhk66" [5fe9904a-ab92-4eb4-a075-dd97140d5780] Running
I1130 21:46:26.094730   30756 system_pods.go:61] "kube-scheduler-minikube" [74f21923-d400-4fbc-8030-510657ccc803] Running
I1130 21:46:26.094730   30756 system_pods.go:61] "storage-provisioner" [9d33a503-5e3a-4089-ac3f-9a057e228d8f] Running / Ready:ContainersNotReady (containers with unready status: [storage-provisioner]) / ContainersReady:ContainersNotReady (containers with unready status: [storage-provisioner])
I1130 21:46:26.094730   30756 system_pods.go:74] duration metric: took 9.8732ms to wait for pod list to return data ...
I1130 21:46:26.094730   30756 kubeadm.go:581] duration metric: took 2.6480677s to wait for : map[apiserver:true system_pods:true] ...
I1130 21:46:26.094730   30756 node_conditions.go:102] verifying NodePressure condition ...
I1130 21:46:26.100228   30756 node_conditions.go:122] node storage ephemeral capacity is 1055762868Ki
I1130 21:46:26.100228   30756 node_conditions.go:123] node cpu capacity is 12
I1130 21:46:26.100228   30756 node_conditions.go:105] duration metric: took 5.4981ms to run NodePressure ...
I1130 21:46:26.100228   30756 start.go:228] waiting for startup goroutines ...
I1130 21:46:26.384377   30756 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.28.3/kubectl apply -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml: (1.6676559s)
I1130 21:46:26.385456   30756 out.go:177] * Some dashboard features require the metrics-server addon. To enable all features please run:

	minikube addons enable metrics-server	


I1130 21:46:26.387053   30756 out.go:177] * Enabled addons: storage-provisioner, default-storageclass, dashboard
I1130 21:46:26.388118   30756 addons.go:502] enable addons completed in 2.9537229s: enabled=[storage-provisioner default-storageclass dashboard]
I1130 21:46:26.388118   30756 start.go:233] waiting for cluster config update ...
I1130 21:46:26.388118   30756 start.go:242] writing updated cluster config ...
I1130 21:46:26.395725   30756 ssh_runner.go:195] Run: rm -f paused
I1130 21:46:26.559734   30756 start.go:600] kubectl: 1.28.2, cluster: 1.28.3 (minor skew: 0)
I1130 21:46:26.561438   30756 out.go:177] * Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default

* 
* ==> Docker <==
* Nov 30 16:16:01 minikube dockerd[632859]: time="2023-11-30T16:16:01.165173640Z" level=info msg="[graphdriver] trying configured driver: overlay2"
Nov 30 16:16:01 minikube dockerd[632859]: time="2023-11-30T16:16:01.190367957Z" level=info msg="Loading containers: start."
Nov 30 16:16:01 minikube dockerd[632859]: time="2023-11-30T16:16:01.439631843Z" level=info msg="Default bridge (docker0) is assigned with an IP address 172.17.0.0/16. Daemon option --bip can be used to set a preferred IP address"
Nov 30 16:16:01 minikube dockerd[632859]: time="2023-11-30T16:16:01.498062028Z" level=info msg="Loading containers: done."
Nov 30 16:16:01 minikube dockerd[632859]: time="2023-11-30T16:16:01.526301775Z" level=warning msg="WARNING: No blkio throttle.read_bps_device support"
Nov 30 16:16:01 minikube dockerd[632859]: time="2023-11-30T16:16:01.526352372Z" level=warning msg="WARNING: No blkio throttle.write_bps_device support"
Nov 30 16:16:01 minikube dockerd[632859]: time="2023-11-30T16:16:01.526360798Z" level=warning msg="WARNING: No blkio throttle.read_iops_device support"
Nov 30 16:16:01 minikube dockerd[632859]: time="2023-11-30T16:16:01.526366609Z" level=warning msg="WARNING: No blkio throttle.write_iops_device support"
Nov 30 16:16:01 minikube dockerd[632859]: time="2023-11-30T16:16:01.526393721Z" level=info msg="Docker daemon" commit=311b9ff graphdriver=overlay2 version=24.0.7
Nov 30 16:16:01 minikube dockerd[632859]: time="2023-11-30T16:16:01.526474025Z" level=info msg="Daemon has completed initialization"
Nov 30 16:16:01 minikube dockerd[632859]: time="2023-11-30T16:16:01.555943110Z" level=info msg="API listen on /var/run/docker.sock"
Nov 30 16:16:01 minikube dockerd[632859]: time="2023-11-30T16:16:01.555981439Z" level=info msg="API listen on [::]:2376"
Nov 30 16:16:01 minikube systemd[1]: Started Docker Application Container Engine.
Nov 30 16:16:01 minikube cri-dockerd[1589]: time="2023-11-30T16:16:01Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"hello-minikube-7f54cff968-9cwt9_default\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"6136584b3a44dbf06d66c6ec3e25b561fd4821a66a15763565da7a91709ecca4\""
Nov 30 16:16:01 minikube cri-dockerd[1589]: time="2023-11-30T16:16:01Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"kubernetes-dashboard-8694d4445c-gzzhn_kubernetes-dashboard\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"4006a1905888371d68046205ab092a18dc0d5ae9e8eb7d18ee6fdb9bc4134137\""
Nov 30 16:16:01 minikube cri-dockerd[1589]: time="2023-11-30T16:16:01Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"dashboard-metrics-scraper-7fd5cb4ddc-49kzf_kubernetes-dashboard\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"ff9873f4d991e0186cac3fbe5b5f6522ff581f3fae94efb7503c121bc7f65794\""
Nov 30 16:16:01 minikube cri-dockerd[1589]: time="2023-11-30T16:16:01Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"placement-app-6498bdb476-89kch_default\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"07029f6eae5b7b2dc3ec0f370e37549a03d2ff86536987698a77c8718155b62d\""
Nov 30 16:16:01 minikube cri-dockerd[1589]: time="2023-11-30T16:16:01Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"coredns-5dd5756b68-mkpn4_kube-system\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"755721c7054606f6ac0028f74df38c856bbd2d037e2d64e584cbecc2cd7cf070\""
Nov 30 16:16:02 minikube systemd[1]: Stopping CRI Interface for Docker Application Container Engine...
Nov 30 16:16:02 minikube systemd[1]: cri-docker.service: Deactivated successfully.
Nov 30 16:16:02 minikube systemd[1]: Stopped CRI Interface for Docker Application Container Engine.
Nov 30 16:16:02 minikube systemd[1]: Starting CRI Interface for Docker Application Container Engine...
Nov 30 16:16:02 minikube systemd[1]: cri-docker.service: Deactivated successfully.
Nov 30 16:16:02 minikube systemd[1]: Stopped CRI Interface for Docker Application Container Engine.
Nov 30 16:16:02 minikube systemd[1]: Starting CRI Interface for Docker Application Container Engine...
Nov 30 16:16:02 minikube cri-dockerd[633111]: time="2023-11-30T16:16:02Z" level=info msg="Connecting to docker on the Endpoint unix:///var/run/docker.sock"
Nov 30 16:16:02 minikube cri-dockerd[633111]: time="2023-11-30T16:16:02Z" level=info msg="Start docker client with request timeout 0s"
Nov 30 16:16:02 minikube cri-dockerd[633111]: time="2023-11-30T16:16:02Z" level=info msg="Hairpin mode is set to hairpin-veth"
Nov 30 16:16:02 minikube cri-dockerd[633111]: time="2023-11-30T16:16:02Z" level=info msg="Loaded network plugin cni"
Nov 30 16:16:02 minikube cri-dockerd[633111]: time="2023-11-30T16:16:02Z" level=info msg="Docker cri networking managed by network plugin cni"
Nov 30 16:16:02 minikube cri-dockerd[633111]: time="2023-11-30T16:16:02Z" level=info msg="Docker Info: &{ID:49a600ba-539f-47a5-a14b-3416204c2bb9 Containers:41 ContainersRunning:0 ContainersPaused:0 ContainersStopped:41 Images:13 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:[] Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:[] Log:[awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6tables:true Debug:false NFd:25 OomKillDisable:true NGoroutines:36 SystemTime:2023-11-30T16:16:02.621004945Z LoggingDriver:json-file CgroupDriver:cgroupfs CgroupVersion:1 NEventsListener:0 KernelVersion:5.15.90.1-microsoft-standard-WSL2 OperatingSystem:Ubuntu 22.04.3 LTS (containerized) OSVersion:22.04 OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:0xc0000cecb0 NCPU:12 MemTotal:7989211136 GenericResources:[] DockerRootDir:/var/lib/docker HTTPProxy: HTTPSProxy: NoProxy:control-plane.minikube.internal Name:minikube Labels:[provider=docker] ExperimentalBuild:false ServerVersion:24.0.7 ClusterStore: ClusterAdvertise: Runtimes:map[io.containerd.runc.v2:{Path:runc Args:[] Shim:<nil>} runc:{Path:runc Args:[] Shim:<nil>}] DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:[] Nodes:0 Managers:0 Cluster:<nil> Warnings:[]} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:61f9fd88f79f081d64d6fa3bb1a0dc71ec870523 Expected:61f9fd88f79f081d64d6fa3bb1a0dc71ec870523} RuncCommit:{ID:v1.1.9-0-gccaecfc Expected:v1.1.9-0-gccaecfc} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=builtin] ProductLicense: DefaultAddressPools:[] Warnings:[WARNING: No blkio throttle.read_bps_device support WARNING: No blkio throttle.write_bps_device support WARNING: No blkio throttle.read_iops_device support WARNING: No blkio throttle.write_iops_device support]}"
Nov 30 16:16:02 minikube cri-dockerd[633111]: time="2023-11-30T16:16:02Z" level=info msg="Setting cgroupDriver cgroupfs"
Nov 30 16:16:02 minikube cri-dockerd[633111]: time="2023-11-30T16:16:02Z" level=info msg="Docker cri received runtime config &RuntimeConfig{NetworkConfig:&NetworkConfig{PodCidr:,},}"
Nov 30 16:16:02 minikube cri-dockerd[633111]: time="2023-11-30T16:16:02Z" level=info msg="Starting the GRPC backend for the Docker CRI interface."
Nov 30 16:16:02 minikube cri-dockerd[633111]: time="2023-11-30T16:16:02Z" level=info msg="Start cri-dockerd grpc backend"
Nov 30 16:16:02 minikube systemd[1]: Started CRI Interface for Docker Application Container Engine.
Nov 30 16:16:05 minikube cri-dockerd[633111]: time="2023-11-30T16:16:05Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"hello-minikube-7f54cff968-9cwt9_default\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"fefe45a69135b35d9df683a6659319644b4629bea888079a9bc31fbca2633c28\""
Nov 30 16:16:07 minikube cri-dockerd[633111]: time="2023-11-30T16:16:07Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/59f12a28a4c7ca822f4eb6cc4229afa3edf1b644d636258ff484be0c28941d81/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Nov 30 16:16:07 minikube cri-dockerd[633111]: time="2023-11-30T16:16:07Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/eead2053b25834d8a1d34d1532b2eb338601a787db18a9b9917ce39b41d5630a/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Nov 30 16:16:07 minikube cri-dockerd[633111]: time="2023-11-30T16:16:07Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/e046c9137633337f2f08f010f2c4d287649db86e1c4ba76d8bbdaaebd4e8a3a4/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Nov 30 16:16:07 minikube cri-dockerd[633111]: time="2023-11-30T16:16:07Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/ea4345d50594ffbc8f473403b8af77f2a61daba83c1a48a6dc7c581c74bfb2c7/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Nov 30 16:16:07 minikube cri-dockerd[633111]: time="2023-11-30T16:16:07Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/daaa6bd42b61f0234e9533c3a0d7a4436e699398ff5df1b816811b2ef3926d17/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Nov 30 16:16:07 minikube cri-dockerd[633111]: time="2023-11-30T16:16:07Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/b4e8f31930bf06c459120af94746ea9b7c3006d9426024777389a174e1d4b9d4/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Nov 30 16:16:09 minikube cri-dockerd[633111]: time="2023-11-30T16:16:09Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/53f95dc634eecd9c731976dfc1a4a3f9a4733085638a45d27879f6195edddcd9/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Nov 30 16:16:09 minikube cri-dockerd[633111]: time="2023-11-30T16:16:09Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/1d66fdfc1c03410b01297c5e93b10fb52aca9ce9f2e25961cfc131d2dfa2580c/resolv.conf as [nameserver 10.96.0.10 search kubernetes-dashboard.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Nov 30 16:16:09 minikube cri-dockerd[633111]: time="2023-11-30T16:16:09Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/364d142ef508e368636c1f6521ae48260479aa50dc59541c6fd42ffc28fcfc8c/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Nov 30 16:16:09 minikube cri-dockerd[633111]: time="2023-11-30T16:16:09Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/8b9ee01c491e4624502a2d1e40d7a2f23abbe852c7ad6338efba338da6a667ea/resolv.conf as [nameserver 10.96.0.10 search kubernetes-dashboard.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Nov 30 16:16:09 minikube cri-dockerd[633111]: time="2023-11-30T16:16:09Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/9e8ef3848c8e05687d6316b5beca151b37d318163761db547203a7db1ff167d6/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Nov 30 16:16:12 minikube dockerd[632859]: time="2023-11-30T16:16:12.196281275Z" level=info msg="ignoring event" container=17e825af3e70b7c1d1e801c09da44d138ca3cc4860f4884a84d36ba17078ba6e module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Nov 30 16:16:14 minikube dockerd[632859]: time="2023-11-30T16:16:14.893641971Z" level=info msg="ignoring event" container=f7fdc317b6a44ccccce157c99d71078a23d58ad4e8722adf173abb89c08bf9a4 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Nov 30 16:16:14 minikube cri-dockerd[633111]: time="2023-11-30T16:16:14Z" level=info msg="Stop pulling image subbu0319/placement-app:latest: Status: Image is up to date for subbu0319/placement-app:latest"
Nov 30 16:32:38 minikube cri-dockerd[633111]: time="2023-11-30T16:32:38Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/4268e589068754a6c5a0047ff299cf5a49c5c66e7db530bf0fde88131307a7c0/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Nov 30 16:32:53 minikube cri-dockerd[633111]: time="2023-11-30T16:32:53Z" level=info msg="Pulling image srisoudamini/churnmodelling:latest: 13baa2029dde: Downloading [========>                                          ]  4.184MB/24.05MB"
Nov 30 16:33:03 minikube cri-dockerd[633111]: time="2023-11-30T16:33:03Z" level=info msg="Pulling image srisoudamini/churnmodelling:latest: 325c5bf4c2f2: Downloading [==================>                                ]  23.64MB/64.13MB"
Nov 30 16:33:13 minikube cri-dockerd[633111]: time="2023-11-30T16:33:13Z" level=info msg="Pulling image srisoudamini/churnmodelling:latest: 325c5bf4c2f2: Downloading [============================>                      ]  36.02MB/64.13MB"
Nov 30 16:33:23 minikube cri-dockerd[633111]: time="2023-11-30T16:33:23Z" level=info msg="Pulling image srisoudamini/churnmodelling:latest: 8457fd5474e7: Downloading [==============>                                    ]  14.69MB/49.58MB"
Nov 30 16:33:33 minikube cri-dockerd[633111]: time="2023-11-30T16:33:33Z" level=info msg="Pulling image srisoudamini/churnmodelling:latest: 13baa2029dde: Downloading [=======================================>           ]  18.95MB/24.05MB"
Nov 30 16:33:43 minikube cri-dockerd[633111]: time="2023-11-30T16:33:43Z" level=info msg="Pulling image srisoudamini/churnmodelling:latest: 7e18a660069f: Downloading [==>                                                ]  9.145MB/211.1MB"
Nov 30 16:33:53 minikube cri-dockerd[633111]: time="2023-11-30T16:33:53Z" level=info msg="Pulling image srisoudamini/churnmodelling:latest: b61148e36140: Downloading [=>                                                 ]  327.7kB/15.82MB"
Nov 30 16:34:03 minikube cri-dockerd[633111]: time="2023-11-30T16:34:03Z" level=info msg="Pulling image srisoudamini/churnmodelling:latest: 8457fd5474e7: Downloading [=============================>                     ]  28.89MB/49.58MB"

* 
* ==> container status <==
* CONTAINER           IMAGE                                                                                                  CREATED             STATE               NAME                        ATTEMPT             POD ID              POD
474d57da9ea5b       07655ddf2eebe                                                                                          17 minutes ago      Running             kubernetes-dashboard        12                  1d66fdfc1c034       kubernetes-dashboard-8694d4445c-gzzhn
95ed8f7414964       6e38f40d628db                                                                                          17 minutes ago      Running             storage-provisioner         14                  59f12a28a4c7c       storage-provisioner
6317a757e1e63       subbu0319/placement-app@sha256:74bdc04f5da33e9150232a40b98ec262c02eecc030de4fc8d971f8a064d3c830        17 minutes ago      Running             placement-app               1                   364d142ef508e       placement-app-6498bdb476-89kch
79759f04ba662       115053965e86b                                                                                          17 minutes ago      Running             dashboard-metrics-scraper   7                   8b9ee01c491e4       dashboard-metrics-scraper-7fd5cb4ddc-49kzf
95cc373d62bd1       9056ab77afb8e                                                                                          17 minutes ago      Running             echo-server                 7                   9e8ef3848c8e0       hello-minikube-7f54cff968-9cwt9
f7fdc317b6a44       07655ddf2eebe                                                                                          17 minutes ago      Exited              kubernetes-dashboard        11                  1d66fdfc1c034       kubernetes-dashboard-8694d4445c-gzzhn
fd138efa969ca       ead0a4a53df89                                                                                          17 minutes ago      Running             coredns                     9                   53f95dc634eec       coredns-5dd5756b68-mkpn4
17e825af3e70b       6e38f40d628db                                                                                          17 minutes ago      Exited              storage-provisioner         13                  59f12a28a4c7c       storage-provisioner
8087b35105fa8       73deb9a3f7025                                                                                          17 minutes ago      Running             etcd                        9                   b4e8f31930bf0       etcd-minikube
923e0d01ba3df       bfc896cf80fba                                                                                          17 minutes ago      Running             kube-proxy                  9                   daaa6bd42b61f       kube-proxy-nhk66
3f72e2e31d33e       6d1b4fd1b182d                                                                                          17 minutes ago      Running             kube-scheduler              9                   ea4345d50594f       kube-scheduler-minikube
8d77bf2b8840b       5374347291230                                                                                          17 minutes ago      Running             kube-apiserver              9                   e046c91376333       kube-apiserver-minikube
bb55d07969ca2       10baa1ca17068                                                                                          17 minutes ago      Running             kube-controller-manager     9                   eead2053b2583       kube-controller-manager-minikube
69396f4637002       subbu0319/placement-app@sha256:74bdc04f5da33e9150232a40b98ec262c02eecc030de4fc8d971f8a064d3c830        2 days ago          Exited              placement-app               0                   07029f6eae5b7       placement-app-6498bdb476-89kch
5e8f07d6cfa7b       kubernetesui/metrics-scraper@sha256:76049887f07a0476dc93efc2d3569b9529bf982b22d29f356092ce206e98765c   2 days ago          Exited              dashboard-metrics-scraper   6                   ff9873f4d991e       dashboard-metrics-scraper-7fd5cb4ddc-49kzf
451f7392e3612       kicbase/echo-server@sha256:127ac38a2bb9537b7f252addff209ea6801edcac8a92c8b1104dacd66a583ed6            2 days ago          Exited              echo-server                 6                   6136584b3a44d       hello-minikube-7f54cff968-9cwt9
f663a3e2acee9       ead0a4a53df89                                                                                          2 days ago          Exited              coredns                     8                   755721c705460       coredns-5dd5756b68-mkpn4
819d082ff3818       bfc896cf80fba                                                                                          2 days ago          Exited              kube-proxy                  8                   282d116da3e40       kube-proxy-nhk66
439d66dd68fce       73deb9a3f7025                                                                                          2 days ago          Exited              etcd                        8                   37377354d9026       etcd-minikube
c04db572bd273       6d1b4fd1b182d                                                                                          2 days ago          Exited              kube-scheduler              8                   527e8a631d24c       kube-scheduler-minikube
85d0195b4002e       10baa1ca17068                                                                                          2 days ago          Exited              kube-controller-manager     8                   69b2117e56cc0       kube-controller-manager-minikube
39bfc316dae25       5374347291230                                                                                          2 days ago          Exited              kube-apiserver              8                   b382b278fe3bb       kube-apiserver-minikube

* 
* ==> coredns [f663a3e2acee] <==
* .:53
[INFO] plugin/reload: Running configuration SHA512 = f869070685748660180df1b7a47d58cdafcf2f368266578c062d1151dc2c900964aecc5975e8882e6de6fdfb6460463e30ebfaad2ec8f0c3c6436f80225b3b5b
CoreDNS-1.10.1
linux/amd64, go1.20, 055b2c3
[INFO] 127.0.0.1:59332 - 17277 "HINFO IN 4319018533358847598.2217384459616060003. udp 57 false 512" NXDOMAIN qr,rd,ra 57 0.205154025s
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 1.898855814s
[INFO] SIGTERM: Shutting down servers then terminating
[INFO] plugin/health: Going into lameduck mode for 5s

* 
* ==> coredns [fd138efa969c] <==
* [INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[WARNING] plugin/kubernetes: starting server with unsynced Kubernetes API
.:53
[INFO] plugin/reload: Running configuration SHA512 = f869070685748660180df1b7a47d58cdafcf2f368266578c062d1151dc2c900964aecc5975e8882e6de6fdfb6460463e30ebfaad2ec8f0c3c6436f80225b3b5b
CoreDNS-1.10.1
linux/amd64, go1.20, 055b2c3
[INFO] 127.0.0.1:58764 - 43885 "HINFO IN 5171966653188387552.8112521453889250843. udp 57 false 512" NXDOMAIN qr,rd,ra 57 0.97351134s

* 
* ==> describe nodes <==
* Name:               minikube
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=8220a6eb95f0a4d75f7f2d7b14cef975f050512d
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2023_11_22T17_31_57_0700
                    minikube.k8s.io/version=v1.32.0
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/cri-dockerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Wed, 22 Nov 2023 12:01:53 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Thu, 30 Nov 2023 16:33:56 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Thu, 30 Nov 2023 16:29:44 +0000   Wed, 22 Nov 2023 12:01:53 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Thu, 30 Nov 2023 16:29:44 +0000   Wed, 22 Nov 2023 12:01:53 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Thu, 30 Nov 2023 16:29:44 +0000   Wed, 22 Nov 2023 12:01:53 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Thu, 30 Nov 2023 16:29:44 +0000   Wed, 22 Nov 2023 12:02:07 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.49.2
  Hostname:    minikube
Capacity:
  cpu:                12
  ephemeral-storage:  1055762868Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             7801964Ki
  pods:               110
Allocatable:
  cpu:                12
  ephemeral-storage:  1055762868Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             7801964Ki
  pods:               110
System Info:
  Machine ID:                 f3676bc9899d47b78129c6bdb30d3cf4
  System UUID:                f3676bc9899d47b78129c6bdb30d3cf4
  Boot ID:                    fa011946-9a3d-47ab-9d95-2cc2c24979d5
  Kernel Version:             5.15.90.1-microsoft-standard-WSL2
  OS Image:                   Ubuntu 22.04.3 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://24.0.7
  Kubelet Version:            v1.28.3
  Kube-Proxy Version:         v1.28.3
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (12 in total)
  Namespace                   Name                                          CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                          ------------  ----------  ---------------  -------------  ---
  default                     churnmodelling-55f7c66779-w4xxz               500m (4%!)(MISSING)     500m (4%!)(MISSING)   128Mi (1%!)(MISSING)       128Mi (1%!)(MISSING)     87s
  default                     hello-minikube-7f54cff968-9cwt9               0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         8d
  default                     placement-app-6498bdb476-89kch                500m (4%!)(MISSING)     500m (4%!)(MISSING)   128Mi (1%!)(MISSING)       128Mi (1%!)(MISSING)     2d1h
  kube-system                 coredns-5dd5756b68-mkpn4                      100m (0%!)(MISSING)     0 (0%!)(MISSING)      70Mi (0%!)(MISSING)        170Mi (2%!)(MISSING)     8d
  kube-system                 etcd-minikube                                 100m (0%!)(MISSING)     0 (0%!)(MISSING)      100Mi (1%!)(MISSING)       0 (0%!)(MISSING)         8d
  kube-system                 kube-apiserver-minikube                       250m (2%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         8d
  kube-system                 kube-controller-manager-minikube              200m (1%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         8d
  kube-system                 kube-proxy-nhk66                              0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         8d
  kube-system                 kube-scheduler-minikube                       100m (0%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         8d
  kube-system                 storage-provisioner                           0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         8d
  kubernetes-dashboard        dashboard-metrics-scraper-7fd5cb4ddc-49kzf    0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         8d
  kubernetes-dashboard        kubernetes-dashboard-8694d4445c-gzzhn         0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         8d
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests     Limits
  --------           --------     ------
  cpu                1750m (14%!)(MISSING)  1 (8%!)(MISSING)
  memory             426Mi (5%!)(MISSING)   426Mi (5%!)(MISSING)
  ephemeral-storage  0 (0%!)(MISSING)       0 (0%!)(MISSING)
  hugepages-1Gi      0 (0%!)(MISSING)       0 (0%!)(MISSING)
  hugepages-2Mi      0 (0%!)(MISSING)       0 (0%!)(MISSING)
Events:
  Type    Reason          Age   From             Message
  ----    ------          ----  ----             -------
  Normal  Starting        17m   kube-proxy       
  Normal  RegisteredNode  17m   node-controller  Node minikube event: Registered Node minikube in Controller

* 
* ==> dmesg <==
* [Nov30 02:53] PCI: Fatal: No config space access function found
[  +0.012406] PCI: System does not support PCI
[  +0.007047] kvm: no hardware support
[  +1.182049] FS-Cache: Duplicate cookie detected
[  +0.000456] FS-Cache: O-cookie c=00000004 [p=00000002 fl=222 nc=0 na=1]
[  +0.000404] FS-Cache: O-cookie d=000000009d732238{9P.session} n=00000000f9a25889
[  +0.000447] FS-Cache: O-key=[10] '34323934393337343137'
[  +0.000343] FS-Cache: N-cookie c=00000005 [p=00000002 fl=2 nc=0 na=1]
[  +0.000443] FS-Cache: N-cookie d=000000009d732238{9P.session} n=0000000095e2690a
[  +0.000524] FS-Cache: N-key=[10] '34323934393337343137'
[  +0.001535] FS-Cache: Duplicate cookie detected
[  +0.000393] FS-Cache: O-cookie c=00000004 [p=00000002 fl=222 nc=0 na=1]
[  +0.000491] FS-Cache: O-cookie d=000000009d732238{9P.session} n=00000000f9a25889
[  +0.000522] FS-Cache: O-key=[10] '34323934393337343137'
[  +0.000378] FS-Cache: N-cookie c=00000006 [p=00000002 fl=2 nc=0 na=1]
[  +0.000420] FS-Cache: N-cookie d=000000009d732238{9P.session} n=00000000515a384c
[  +0.001031] FS-Cache: N-key=[10] '34323934393337343137'
[  +0.976247] 9pnet_virtio: no channels available for device drvfs
[  +0.000954] WSL (1) WARNING: mount: waiting for virtio device drvfs
[  +0.158741] WSL (1) ERROR: ConfigApplyWindowsLibPath:2431: open /etc/ld.so.conf.d/ld.wsl.conf
[  +0.000005]  failed 2
[  +0.010832] 9pnet_virtio: no channels available for device drvfs
[  +0.000565] WSL (1) WARNING: mount: waiting for virtio device drvfs
[  +0.110375] WSL (1) WARNING: /usr/share/zoneinfo/Asia/Calcutta not found. Is the tzdata package installed?
[  +0.179335] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000762] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000583] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000716] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.293127] 9pnet_virtio: no channels available for device drvfs
[  +0.000545] WSL (1) WARNING: mount: waiting for virtio device drvfs
[  +0.206326] WSL (2) ERROR: UtilCreateProcessAndWait:662: /bin/mount failed with 2
[  +0.000956] WSL (1) ERROR: UtilCreateProcessAndWait:684: /bin/mount failed with status 0xff00

[  +0.001165] WSL (1) ERROR: ConfigMountFsTab:2483: Processing fstab with mount -a failed.
[  +0.002165] WSL (1) ERROR: ConfigApplyWindowsLibPath:2431: open /etc/ld.so.conf.d/ld.wsl.conf
[  +0.000003]  failed 2
[  +0.011875] 9pnet_virtio: no channels available for device drvfs
[  +0.000672] WSL (1) WARNING: mount: waiting for virtio device drvfs
[  +0.100723] 9pnet_virtio: no channels available for device drvfs
[  +0.014943] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000824] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000655] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000761] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.193947] WSL (1) WARNING: /usr/share/zoneinfo/Asia/Calcutta not found. Is the tzdata package installed?
[  +0.583592] netlink: 'init': attribute type 4 has an invalid length.
[  +0.719896] kmem.limit_in_bytes is deprecated and will be removed. Please report your usecase to linux-mm@kvack.org if you depend on this functionality.

* 
* ==> etcd [439d66dd68fc] <==
* {"level":"info","ts":"2023-11-30T14:17:24.887115Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3941024354,"revision":39407,"compact-revision":39167}
{"level":"info","ts":"2023-11-30T14:21:14.666899Z","caller":"traceutil/trace.go:171","msg":"trace[2076774871] transaction","detail":"{read_only:false; response_revision:39827; number_of_response:1; }","duration":"107.411915ms","start":"2023-11-30T14:21:14.559467Z","end":"2023-11-30T14:21:14.666879Z","steps":["trace[2076774871] 'process raft request'  (duration: 107.252963ms)"],"step_count":1}
{"level":"info","ts":"2023-11-30T14:22:24.9048Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":39645}
{"level":"info","ts":"2023-11-30T14:22:24.906229Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":39645,"took":"1.188763ms","hash":2039434244}
{"level":"info","ts":"2023-11-30T14:22:24.906302Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2039434244,"revision":39645,"compact-revision":39407}
{"level":"info","ts":"2023-11-30T14:25:19.795192Z","caller":"etcdserver/server.go:1395","msg":"triggering snapshot","local-member-id":"aec36adc501070cc","local-member-applied-index":50005,"local-member-snapshot-index":40004,"local-member-snapshot-count":10000}
{"level":"info","ts":"2023-11-30T14:25:19.808266Z","caller":"etcdserver/server.go:2413","msg":"saved snapshot","snapshot-index":50005}
{"level":"info","ts":"2023-11-30T14:25:19.809006Z","caller":"etcdserver/server.go:2443","msg":"compacted Raft logs","compact-index":45005}
{"level":"info","ts":"2023-11-30T14:27:24.928483Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":39884}
{"level":"info","ts":"2023-11-30T14:27:24.929889Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":39884,"took":"1.092916ms","hash":3124551849}
{"level":"info","ts":"2023-11-30T14:27:24.929955Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3124551849,"revision":39884,"compact-revision":39645}
{"level":"info","ts":"2023-11-30T14:32:24.950992Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":40124}
{"level":"info","ts":"2023-11-30T14:32:24.952882Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":40124,"took":"1.616386ms","hash":1308568271}
{"level":"info","ts":"2023-11-30T14:32:24.952958Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1308568271,"revision":40124,"compact-revision":39884}
{"level":"info","ts":"2023-11-30T14:37:24.972243Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":40361}
{"level":"info","ts":"2023-11-30T14:37:24.9736Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":40361,"took":"1.098997ms","hash":3393828155}
{"level":"info","ts":"2023-11-30T14:37:24.973659Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3393828155,"revision":40361,"compact-revision":40124}
{"level":"info","ts":"2023-11-30T14:39:42.767893Z","caller":"traceutil/trace.go:171","msg":"trace[1593422238] transaction","detail":"{read_only:false; response_revision:40711; number_of_response:1; }","duration":"102.682043ms","start":"2023-11-30T14:39:42.665186Z","end":"2023-11-30T14:39:42.767868Z","steps":["trace[1593422238] 'process raft request'  (duration: 102.535173ms)"],"step_count":1}
{"level":"info","ts":"2023-11-30T14:42:24.994069Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":40600}
{"level":"info","ts":"2023-11-30T14:42:24.995632Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":40600,"took":"1.223959ms","hash":2462371891}
{"level":"info","ts":"2023-11-30T14:42:24.995694Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2462371891,"revision":40600,"compact-revision":40361}
{"level":"info","ts":"2023-11-30T14:46:53.862739Z","caller":"traceutil/trace.go:171","msg":"trace[1284328998] transaction","detail":"{read_only:false; response_revision:41053; number_of_response:1; }","duration":"106.456373ms","start":"2023-11-30T14:46:53.756258Z","end":"2023-11-30T14:46:53.862715Z","steps":["trace[1284328998] 'process raft request'  (duration: 106.279739ms)"],"step_count":1}
{"level":"info","ts":"2023-11-30T14:47:25.015661Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":40840}
{"level":"info","ts":"2023-11-30T14:47:25.016759Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":40840,"took":"737.563s","hash":334982657}
{"level":"info","ts":"2023-11-30T14:47:25.016806Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":334982657,"revision":40840,"compact-revision":40600}
{"level":"info","ts":"2023-11-30T14:52:01.058047Z","caller":"traceutil/trace.go:171","msg":"trace[78111981] transaction","detail":"{read_only:false; response_revision:41298; number_of_response:1; }","duration":"102.718039ms","start":"2023-11-30T14:52:00.955305Z","end":"2023-11-30T14:52:01.058023Z","steps":["trace[78111981] 'process raft request'  (duration: 102.566081ms)"],"step_count":1}
{"level":"info","ts":"2023-11-30T14:52:11.351185Z","caller":"traceutil/trace.go:171","msg":"trace[1137660863] transaction","detail":"{read_only:false; response_revision:41306; number_of_response:1; }","duration":"100.562351ms","start":"2023-11-30T14:52:11.250602Z","end":"2023-11-30T14:52:11.351164Z","steps":["trace[1137660863] 'process raft request'  (duration: 100.252882ms)"],"step_count":1}
{"level":"info","ts":"2023-11-30T14:52:21.555482Z","caller":"traceutil/trace.go:171","msg":"trace[1844261047] transaction","detail":"{read_only:false; response_revision:41314; number_of_response:1; }","duration":"101.616616ms","start":"2023-11-30T14:52:21.453838Z","end":"2023-11-30T14:52:21.555455Z","steps":["trace[1844261047] 'process raft request'  (duration: 101.42982ms)"],"step_count":1}
{"level":"info","ts":"2023-11-30T14:52:25.035861Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":41078}
{"level":"info","ts":"2023-11-30T14:52:25.037401Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":41078,"took":"1.261284ms","hash":1055666921}
{"level":"info","ts":"2023-11-30T14:52:25.037471Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1055666921,"revision":41078,"compact-revision":40840}
{"level":"info","ts":"2023-11-30T14:55:05.566299Z","caller":"traceutil/trace.go:171","msg":"trace[228510334] transaction","detail":"{read_only:false; response_revision:41446; number_of_response:1; }","duration":"110.990537ms","start":"2023-11-30T14:55:05.455275Z","end":"2023-11-30T14:55:05.566266Z","steps":["trace[228510334] 'process raft request'  (duration: 110.827387ms)"],"step_count":1}
{"level":"info","ts":"2023-11-30T14:57:25.046081Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":41318}
{"level":"info","ts":"2023-11-30T14:57:25.052808Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":41318,"took":"3.627296ms","hash":3491508098}
{"level":"info","ts":"2023-11-30T14:57:25.052886Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3491508098,"revision":41318,"compact-revision":41078}
{"level":"info","ts":"2023-11-30T15:02:25.070308Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":41557}
{"level":"info","ts":"2023-11-30T15:02:25.07242Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":41557,"took":"1.781765ms","hash":862570100}
{"level":"info","ts":"2023-11-30T15:02:25.072484Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":862570100,"revision":41557,"compact-revision":41318}
{"level":"info","ts":"2023-11-30T15:02:26.156308Z","caller":"traceutil/trace.go:171","msg":"trace[379392053] transaction","detail":"{read_only:false; response_revision:41796; number_of_response:1; }","duration":"106.694195ms","start":"2023-11-30T15:02:26.049585Z","end":"2023-11-30T15:02:26.15628Z","steps":["trace[379392053] 'process raft request'  (duration: 106.442603ms)"],"step_count":1}
{"level":"info","ts":"2023-11-30T15:05:00.256631Z","caller":"traceutil/trace.go:171","msg":"trace[283480782] transaction","detail":"{read_only:false; response_revision:41920; number_of_response:1; }","duration":"100.892019ms","start":"2023-11-30T15:05:00.155712Z","end":"2023-11-30T15:05:00.256604Z","steps":["trace[283480782] 'process raft request'  (duration: 100.582258ms)"],"step_count":1}
{"level":"info","ts":"2023-11-30T15:07:25.092079Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":41795}
{"level":"info","ts":"2023-11-30T15:07:25.093282Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":41795,"took":"916.497s","hash":1031106897}
{"level":"info","ts":"2023-11-30T15:07:25.093333Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1031106897,"revision":41795,"compact-revision":41557}
{"level":"info","ts":"2023-11-30T15:12:25.110312Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":42035}
{"level":"info","ts":"2023-11-30T15:12:25.111706Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":42035,"took":"1.15885ms","hash":1443169168}
{"level":"info","ts":"2023-11-30T15:12:25.111758Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1443169168,"revision":42035,"compact-revision":41795}
{"level":"info","ts":"2023-11-30T16:12:27.791848Z","caller":"traceutil/trace.go:171","msg":"trace[1370536049] transaction","detail":"{read_only:false; response_revision:42475; number_of_response:1; }","duration":"105.357676ms","start":"2023-11-30T16:12:27.686466Z","end":"2023-11-30T16:12:27.791823Z","steps":["trace[1370536049] 'process raft request'  (duration: 13.505229ms)","trace[1370536049] 'compare'  (duration: 91.714314ms)"],"step_count":2}
{"level":"info","ts":"2023-11-30T16:13:14.768011Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":42273}
{"level":"info","ts":"2023-11-30T16:13:14.769392Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":42273,"took":"1.079262ms","hash":56565711}
{"level":"info","ts":"2023-11-30T16:13:14.769441Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":56565711,"revision":42273,"compact-revision":42035}
{"level":"info","ts":"2023-11-30T16:15:48.39052Z","caller":"osutil/interrupt_unix.go:64","msg":"received signal; shutting down","signal":"terminated"}
{"level":"info","ts":"2023-11-30T16:15:48.394658Z","caller":"embed/etcd.go:376","msg":"closing etcd server","name":"minikube","data-dir":"/var/lib/minikube/etcd","advertise-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"]}
{"level":"warn","ts":"2023-11-30T16:15:48.399602Z","caller":"embed/serve.go:212","msg":"stopping secure grpc server due to error","error":"accept tcp 127.0.0.1:2379: use of closed network connection"}
{"level":"warn","ts":"2023-11-30T16:15:48.401903Z","caller":"embed/serve.go:214","msg":"stopped secure grpc server due to error","error":"accept tcp 127.0.0.1:2379: use of closed network connection"}
{"level":"warn","ts":"2023-11-30T16:15:48.697836Z","caller":"embed/serve.go:212","msg":"stopping secure grpc server due to error","error":"accept tcp 192.168.49.2:2379: use of closed network connection"}
{"level":"warn","ts":"2023-11-30T16:15:48.697947Z","caller":"embed/serve.go:214","msg":"stopped secure grpc server due to error","error":"accept tcp 192.168.49.2:2379: use of closed network connection"}
{"level":"info","ts":"2023-11-30T16:15:48.699388Z","caller":"etcdserver/server.go:1465","msg":"skipped leadership transfer for single voting member cluster","local-member-id":"aec36adc501070cc","current-leader-member-id":"aec36adc501070cc"}
{"level":"info","ts":"2023-11-30T16:15:48.992991Z","caller":"embed/etcd.go:579","msg":"stopping serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2023-11-30T16:15:49.086084Z","caller":"embed/etcd.go:584","msg":"stopped serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2023-11-30T16:15:49.086242Z","caller":"embed/etcd.go:378","msg":"closed etcd server","name":"minikube","data-dir":"/var/lib/minikube/etcd","advertise-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"]}

* 
* ==> etcd [8087b35105fa] <==
* {"level":"info","ts":"2023-11-30T16:16:19.197542Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"raft.node: aec36adc501070cc elected leader aec36adc501070cc at term 11"}
{"level":"info","ts":"2023-11-30T16:16:19.207364Z","caller":"etcdserver/server.go:2062","msg":"published local member to cluster through raft","local-member-id":"aec36adc501070cc","local-member-attributes":"{Name:minikube ClientURLs:[https://192.168.49.2:2379]}","request-path":"/0/members/aec36adc501070cc/attributes","cluster-id":"fa54960ea34d58be","publish-timeout":"7s"}
{"level":"info","ts":"2023-11-30T16:16:19.207399Z","caller":"embed/serve.go:103","msg":"ready to serve client requests"}
{"level":"info","ts":"2023-11-30T16:16:19.207437Z","caller":"embed/serve.go:103","msg":"ready to serve client requests"}
{"level":"info","ts":"2023-11-30T16:16:19.208288Z","caller":"etcdmain/main.go:44","msg":"notifying init daemon"}
{"level":"info","ts":"2023-11-30T16:16:19.208336Z","caller":"etcdmain/main.go:50","msg":"successfully notified init daemon"}
{"level":"info","ts":"2023-11-30T16:16:19.213202Z","caller":"embed/serve.go:250","msg":"serving client traffic securely","traffic":"grpc+http","address":"127.0.0.1:2379"}
{"level":"info","ts":"2023-11-30T16:16:19.213219Z","caller":"embed/serve.go:250","msg":"serving client traffic securely","traffic":"grpc+http","address":"192.168.49.2:2379"}
{"level":"info","ts":"2023-11-30T16:17:53.840934Z","caller":"traceutil/trace.go:171","msg":"trace[882894186] transaction","detail":"{read_only:false; response_revision:42831; number_of_response:1; }","duration":"223.315935ms","start":"2023-11-30T16:17:53.608875Z","end":"2023-11-30T16:17:53.832191Z","steps":["trace[882894186] 'process raft request'  (duration: 223.121582ms)"],"step_count":1}
{"level":"warn","ts":"2023-11-30T16:17:55.911396Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"1.432045174s","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/flowschemas/\" range_end:\"/registry/flowschemas0\" count_only:true ","response":"range_response_count:0 size:8"}
{"level":"warn","ts":"2023-11-30T16:17:55.911499Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"1.222961303s","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" ","response":"range_response_count:1 size:1112"}
{"level":"info","ts":"2023-11-30T16:17:55.911535Z","caller":"traceutil/trace.go:171","msg":"trace[490254542] range","detail":"{range_begin:/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath; range_end:; response_count:1; response_revision:42831; }","duration":"1.224035654s","start":"2023-11-30T16:17:54.687487Z","end":"2023-11-30T16:17:55.911523Z","steps":["trace[490254542] 'range keys from in-memory index tree'  (duration: 1.222810294s)"],"step_count":1}
{"level":"info","ts":"2023-11-30T16:17:55.911496Z","caller":"traceutil/trace.go:171","msg":"trace[835928635] range","detail":"{range_begin:/registry/flowschemas/; range_end:/registry/flowschemas0; response_count:0; response_revision:42831; }","duration":"1.43315104s","start":"2023-11-30T16:17:54.478329Z","end":"2023-11-30T16:17:55.91148Z","steps":["trace[835928635] 'count revisions from in-memory index tree'  (duration: 1.431887062s)"],"step_count":1}
{"level":"warn","ts":"2023-11-30T16:17:55.911402Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"1.028677662s","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2023-11-30T16:17:55.911951Z","caller":"traceutil/trace.go:171","msg":"trace[963989413] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:42831; }","duration":"1.030156724s","start":"2023-11-30T16:17:54.88169Z","end":"2023-11-30T16:17:55.911847Z","steps":["trace[963989413] 'range keys from in-memory index tree'  (duration: 1.028565316s)"],"step_count":1}
{"level":"warn","ts":"2023-11-30T16:17:55.912038Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2023-11-30T16:17:54.881671Z","time spent":"1.03035263s","remote":"127.0.0.1:43540","response type":"/etcdserverpb.KV/Range","request count":0,"request size":18,"response count":0,"response size":30,"request content":"key:\"/registry/health\" "}
{"level":"warn","ts":"2023-11-30T16:17:55.911776Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2023-11-30T16:17:54.478313Z","time spent":"1.433426993s","remote":"127.0.0.1:43940","response type":"/etcdserverpb.KV/Range","request count":0,"request size":50,"response count":13,"response size":32,"request content":"key:\"/registry/flowschemas/\" range_end:\"/registry/flowschemas0\" count_only:true "}
{"level":"warn","ts":"2023-11-30T16:17:55.911572Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2023-11-30T16:17:54.687467Z","time spent":"1.22409142s","remote":"127.0.0.1:43694","response type":"/etcdserverpb.KV/Range","request count":0,"request size":67,"response count":1,"response size":1136,"request content":"key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" "}
{"level":"warn","ts":"2023-11-30T16:17:56.423835Z","caller":"etcdserver/v3_server.go:897","msg":"waiting for ReadIndex response took too long, retrying","sent-request-id":8128025500930636461,"retry-timeout":"500ms"}
{"level":"info","ts":"2023-11-30T16:17:56.597885Z","caller":"traceutil/trace.go:171","msg":"trace[1036711925] linearizableReadLoop","detail":"{readStateIndex:53504; appliedIndex:53503; }","duration":"675.09984ms","start":"2023-11-30T16:17:55.922763Z","end":"2023-11-30T16:17:56.597863Z","steps":["trace[1036711925] 'read index received'  (duration: 674.847968ms)","trace[1036711925] 'applied index is now lower than readState.Index'  (duration: 251s)"],"step_count":2}
{"level":"info","ts":"2023-11-30T16:17:56.59801Z","caller":"traceutil/trace.go:171","msg":"trace[930164353] transaction","detail":"{read_only:false; response_revision:42832; number_of_response:1; }","duration":"677.97348ms","start":"2023-11-30T16:17:55.92002Z","end":"2023-11-30T16:17:56.597993Z","steps":["trace[930164353] 'process raft request'  (duration: 677.669161ms)"],"step_count":1}
{"level":"warn","ts":"2023-11-30T16:17:56.598071Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"675.302898ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2023-11-30T16:17:56.598108Z","caller":"traceutil/trace.go:171","msg":"trace[1923903895] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:42832; }","duration":"675.355559ms","start":"2023-11-30T16:17:55.922741Z","end":"2023-11-30T16:17:56.598096Z","steps":["trace[1923903895] 'agreement among raft nodes before linearized reading'  (duration: 675.256209ms)"],"step_count":1}
{"level":"warn","ts":"2023-11-30T16:17:56.598164Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2023-11-30T16:17:55.922731Z","time spent":"675.422928ms","remote":"127.0.0.1:43540","response type":"/etcdserverpb.KV/Range","request count":0,"request size":18,"response count":0,"response size":30,"request content":"key:\"/registry/health\" "}
{"level":"warn","ts":"2023-11-30T16:17:56.787978Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2023-11-30T16:17:55.919998Z","time spent":"678.054346ms","remote":"127.0.0.1:43694","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":1093,"response count":0,"response size":42,"request content":"compare:<target:MOD key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" mod_revision:42828 > success:<request_put:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" value_size:1020 >> failure:<request_range:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" > >"}
{"level":"info","ts":"2023-11-30T16:18:14.352006Z","caller":"traceutil/trace.go:171","msg":"trace[1552533313] transaction","detail":"{read_only:false; response_revision:42846; number_of_response:1; }","duration":"102.313922ms","start":"2023-11-30T16:18:14.249658Z","end":"2023-11-30T16:18:14.351972Z","steps":["trace[1552533313] 'process raft request'  (duration: 102.057404ms)"],"step_count":1}
{"level":"warn","ts":"2023-11-30T16:18:33.126345Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"263.224149ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2023-11-30T16:18:33.126345Z","caller":"traceutil/trace.go:171","msg":"trace[620011877] linearizableReadLoop","detail":"{readStateIndex:53538; appliedIndex:53537; }","duration":"263.120462ms","start":"2023-11-30T16:18:32.863115Z","end":"2023-11-30T16:18:33.126236Z","steps":["trace[620011877] 'read index received'  (duration: 263.043241ms)","trace[620011877] 'applied index is now lower than readState.Index'  (duration: 75.278s)"],"step_count":2}
{"level":"warn","ts":"2023-11-30T16:18:33.126384Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2023-11-30T16:18:32.714744Z","time spent":"411.636262ms","remote":"127.0.0.1:43584","response type":"/etcdserverpb.Lease/LeaseGrant","request count":-1,"request size":-1,"response count":-1,"response size":-1,"request content":""}
{"level":"info","ts":"2023-11-30T16:18:33.12645Z","caller":"traceutil/trace.go:171","msg":"trace[2134875185] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:42858; }","duration":"263.343317ms","start":"2023-11-30T16:18:32.86309Z","end":"2023-11-30T16:18:33.126434Z","steps":["trace[2134875185] 'agreement among raft nodes before linearized reading'  (duration: 263.164346ms)"],"step_count":1}
{"level":"warn","ts":"2023-11-30T16:18:33.429735Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"135.856175ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128025500930636608 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/masterleases/192.168.49.2\" mod_revision:42851 > success:<request_put:<key:\"/registry/masterleases/192.168.49.2\" value_size:65 lease:8128025500930636605 >> failure:<request_range:<key:\"/registry/masterleases/192.168.49.2\" > >>","response":"size:18"}
{"level":"info","ts":"2023-11-30T16:18:33.429856Z","caller":"traceutil/trace.go:171","msg":"trace[1408015588] linearizableReadLoop","detail":"{readStateIndex:53539; appliedIndex:53538; }","duration":"302.348903ms","start":"2023-11-30T16:18:33.127491Z","end":"2023-11-30T16:18:33.42984Z","steps":["trace[1408015588] 'read index received'  (duration: 165.504411ms)","trace[1408015588] 'applied index is now lower than readState.Index'  (duration: 136.843039ms)"],"step_count":2}
{"level":"info","ts":"2023-11-30T16:18:33.429903Z","caller":"traceutil/trace.go:171","msg":"trace[1394003836] transaction","detail":"{read_only:false; response_revision:42859; number_of_response:1; }","duration":"302.4724ms","start":"2023-11-30T16:18:33.127412Z","end":"2023-11-30T16:18:33.429885Z","steps":["trace[1394003836] 'process raft request'  (duration: 165.65764ms)","trace[1394003836] 'compare'  (duration: 135.73322ms)"],"step_count":2}
{"level":"warn","ts":"2023-11-30T16:18:33.429995Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2023-11-30T16:18:33.127389Z","time spent":"302.567912ms","remote":"127.0.0.1:43584","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":116,"response count":0,"response size":42,"request content":"compare:<target:MOD key:\"/registry/masterleases/192.168.49.2\" mod_revision:42851 > success:<request_put:<key:\"/registry/masterleases/192.168.49.2\" value_size:65 lease:8128025500930636605 >> failure:<request_range:<key:\"/registry/masterleases/192.168.49.2\" > >"}
{"level":"warn","ts":"2023-11-30T16:18:33.430029Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"302.529492ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2023-11-30T16:18:33.430065Z","caller":"traceutil/trace.go:171","msg":"trace[1149797301] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:42859; }","duration":"302.592953ms","start":"2023-11-30T16:18:33.127461Z","end":"2023-11-30T16:18:33.430054Z","steps":["trace[1149797301] 'agreement among raft nodes before linearized reading'  (duration: 302.48105ms)"],"step_count":1}
{"level":"warn","ts":"2023-11-30T16:18:33.430092Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2023-11-30T16:18:33.12745Z","time spent":"302.635694ms","remote":"127.0.0.1:43534","response type":"/etcdserverpb.KV/Range","request count":0,"request size":18,"response count":0,"response size":30,"request content":"key:\"/registry/health\" "}
{"level":"warn","ts":"2023-11-30T16:18:33.745621Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"144.733706ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128025500930636614 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" mod_revision:42858 > success:<request_put:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" value_size:1020 >> failure:<request_range:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" > >>","response":"size:18"}
{"level":"info","ts":"2023-11-30T16:18:33.745803Z","caller":"traceutil/trace.go:171","msg":"trace[436503070] transaction","detail":"{read_only:false; response_revision:42861; number_of_response:1; }","duration":"206.109377ms","start":"2023-11-30T16:18:33.539682Z","end":"2023-11-30T16:18:33.745791Z","steps":["trace[436503070] 'process raft request'  (duration: 206.050976ms)"],"step_count":1}
{"level":"info","ts":"2023-11-30T16:18:33.745803Z","caller":"traceutil/trace.go:171","msg":"trace[525839028] transaction","detail":"{read_only:false; response_revision:42860; number_of_response:1; }","duration":"310.546519ms","start":"2023-11-30T16:18:33.435236Z","end":"2023-11-30T16:18:33.745783Z","steps":["trace[525839028] 'process raft request'  (duration: 165.542646ms)","trace[525839028] 'compare'  (duration: 144.599289ms)"],"step_count":2}
{"level":"warn","ts":"2023-11-30T16:18:33.745983Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2023-11-30T16:18:33.435219Z","time spent":"310.720857ms","remote":"127.0.0.1:43694","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":1093,"response count":0,"response size":42,"request content":"compare:<target:MOD key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" mod_revision:42858 > success:<request_put:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" value_size:1020 >> failure:<request_range:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" > >"}
{"level":"info","ts":"2023-11-30T16:18:56.109832Z","caller":"traceutil/trace.go:171","msg":"trace[1177074223] transaction","detail":"{read_only:false; response_revision:42879; number_of_response:1; }","duration":"104.574941ms","start":"2023-11-30T16:18:56.005236Z","end":"2023-11-30T16:18:56.109811Z","steps":["trace[1177074223] 'process raft request'  (duration: 104.410909ms)"],"step_count":1}
{"level":"warn","ts":"2023-11-30T16:22:07.993913Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"107.94028ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2023-11-30T16:22:07.994082Z","caller":"traceutil/trace.go:171","msg":"trace[1356738898] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:43029; }","duration":"108.156273ms","start":"2023-11-30T16:22:07.885897Z","end":"2023-11-30T16:22:07.994053Z","steps":["trace[1356738898] 'range keys from in-memory index tree'  (duration: 107.661126ms)"],"step_count":1}
{"level":"info","ts":"2023-11-30T16:23:10.097666Z","caller":"traceutil/trace.go:171","msg":"trace[677003491] transaction","detail":"{read_only:false; response_revision:43079; number_of_response:1; }","duration":"170.252918ms","start":"2023-11-30T16:23:09.926992Z","end":"2023-11-30T16:23:10.097245Z","steps":["trace[677003491] 'process raft request'  (duration: 158.468837ms)","trace[677003491] 'store kv pair into bolt db' {req_type:put; key:/registry/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii; req_size:670; } (duration: 11.020699ms)"],"step_count":2}
{"level":"info","ts":"2023-11-30T16:26:19.268088Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":42992}
{"level":"info","ts":"2023-11-30T16:26:19.401271Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":42992,"took":"132.51493ms","hash":267944338}
{"level":"info","ts":"2023-11-30T16:26:19.401342Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":267944338,"revision":42992,"compact-revision":42273}
{"level":"info","ts":"2023-11-30T16:27:07.002627Z","caller":"traceutil/trace.go:171","msg":"trace[814377672] linearizableReadLoop","detail":"{readStateIndex:54053; appliedIndex:54052; }","duration":"105.763472ms","start":"2023-11-30T16:27:06.896838Z","end":"2023-11-30T16:27:07.002601Z","steps":["trace[814377672] 'read index received'  (duration: 105.501243ms)","trace[814377672] 'applied index is now lower than readState.Index'  (duration: 261.077s)"],"step_count":2}
{"level":"info","ts":"2023-11-30T16:27:07.002831Z","caller":"traceutil/trace.go:171","msg":"trace[1062089327] transaction","detail":"{read_only:false; response_revision:43269; number_of_response:1; }","duration":"107.733126ms","start":"2023-11-30T16:27:06.895055Z","end":"2023-11-30T16:27:07.002788Z","steps":["trace[1062089327] 'process raft request'  (duration: 107.349396ms)"],"step_count":1}
{"level":"warn","ts":"2023-11-30T16:27:07.002913Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"100.577161ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:6"}
{"level":"warn","ts":"2023-11-30T16:27:07.002967Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"106.080164ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/services/endpoints/kubernetes-dashboard/dashboard-metrics-scraper\" ","response":"range_response_count:1 size:890"}
{"level":"info","ts":"2023-11-30T16:27:07.002996Z","caller":"traceutil/trace.go:171","msg":"trace[668622708] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:43269; }","duration":"100.672683ms","start":"2023-11-30T16:27:06.902306Z","end":"2023-11-30T16:27:07.002978Z","steps":["trace[668622708] 'agreement among raft nodes before linearized reading'  (duration: 100.550781ms)"],"step_count":1}
{"level":"info","ts":"2023-11-30T16:27:07.003018Z","caller":"traceutil/trace.go:171","msg":"trace[1686491342] range","detail":"{range_begin:/registry/services/endpoints/kubernetes-dashboard/dashboard-metrics-scraper; range_end:; response_count:1; response_revision:43269; }","duration":"106.186118ms","start":"2023-11-30T16:27:06.896805Z","end":"2023-11-30T16:27:07.002991Z","steps":["trace[1686491342] 'agreement among raft nodes before linearized reading'  (duration: 105.967109ms)"],"step_count":1}
{"level":"info","ts":"2023-11-30T16:27:17.395755Z","caller":"traceutil/trace.go:171","msg":"trace[571099143] transaction","detail":"{read_only:false; response_revision:43277; number_of_response:1; }","duration":"108.100484ms","start":"2023-11-30T16:27:17.287621Z","end":"2023-11-30T16:27:17.395721Z","steps":["trace[571099143] 'process raft request'  (duration: 107.867473ms)"],"step_count":1}
{"level":"info","ts":"2023-11-30T16:31:19.290621Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":43230}
{"level":"info","ts":"2023-11-30T16:31:19.292072Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":43230,"took":"1.082511ms","hash":1851772033}
{"level":"info","ts":"2023-11-30T16:31:19.292122Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1851772033,"revision":43230,"compact-revision":42992}
{"level":"info","ts":"2023-11-30T16:31:55.703351Z","caller":"traceutil/trace.go:171","msg":"trace[829711806] transaction","detail":"{read_only:false; response_revision:43499; number_of_response:1; }","duration":"108.9423ms","start":"2023-11-30T16:31:55.594386Z","end":"2023-11-30T16:31:55.703328Z","steps":["trace[829711806] 'process raft request'  (duration: 108.750026ms)"],"step_count":1}
{"level":"info","ts":"2023-11-30T16:32:05.989304Z","caller":"traceutil/trace.go:171","msg":"trace[1666803424] transaction","detail":"{read_only:false; response_revision:43507; number_of_response:1; }","duration":"197.169005ms","start":"2023-11-30T16:32:05.792103Z","end":"2023-11-30T16:32:05.989272Z","steps":["trace[1666803424] 'process raft request'  (duration: 196.9237ms)"],"step_count":1}

* 
* ==> kernel <==
*  16:34:04 up 13:40,  0 users,  load average: 1.82, 0.81, 0.56
Linux minikube 5.15.90.1-microsoft-standard-WSL2 #1 SMP Fri Jan 27 02:56:13 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux
PRETTY_NAME="Ubuntu 22.04.3 LTS"

* 
* ==> kube-apiserver [39bfc316dae2] <==
*   "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1130 16:15:58.478743       1 logging.go:59] [core] [Channel #127 SubChannel #128] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1130 16:15:58.484410       1 logging.go:59] [core] [Channel #70 SubChannel #71] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1130 16:15:58.566083       1 logging.go:59] [core] [Channel #37 SubChannel #38] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1130 16:15:58.605318       1 logging.go:59] [core] [Channel #88 SubChannel #89] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1130 16:15:58.657772       1 logging.go:59] [core] [Channel #118 SubChannel #119] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1130 16:15:58.764195       1 logging.go:59] [core] [Channel #112 SubChannel #113] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1130 16:15:58.881914       1 logging.go:59] [core] [Channel #52 SubChannel #53] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"

* 
* ==> kube-apiserver [8d77bf2b8840] <==
* I1130 16:16:21.283805       1 available_controller.go:423] Starting AvailableConditionController
I1130 16:16:21.283877       1 cache.go:32] Waiting for caches to sync for AvailableConditionController controller
I1130 16:16:21.283965       1 controller.go:78] Starting OpenAPI AggregationController
I1130 16:16:21.284446       1 apiservice_controller.go:97] Starting APIServiceRegistrationController
I1130 16:16:21.284494       1 cache.go:32] Waiting for caches to sync for APIServiceRegistrationController controller
I1130 16:16:21.284467       1 apf_controller.go:372] Starting API Priority and Fairness config controller
I1130 16:16:21.284470       1 aggregator.go:164] waiting for initial CRD sync...
I1130 16:16:21.284464       1 dynamic_serving_content.go:132] "Starting controller" name="aggregator-proxy-cert::/var/lib/minikube/certs/front-proxy-client.crt::/var/lib/minikube/certs/front-proxy-client.key"
I1130 16:16:21.284592       1 handler_discovery.go:412] Starting ResourceDiscoveryManager
I1130 16:16:21.284813       1 controller.go:134] Starting OpenAPI controller
I1130 16:16:21.285090       1 establishing_controller.go:76] Starting EstablishingController
I1130 16:16:21.285112       1 controller.go:85] Starting OpenAPI V3 controller
I1130 16:16:21.285131       1 naming_controller.go:291] Starting NamingConditionController
I1130 16:16:21.285608       1 customresource_discovery_controller.go:289] Starting DiscoveryController
I1130 16:16:21.285641       1 nonstructuralschema_controller.go:192] Starting NonStructuralSchemaConditionController
I1130 16:16:21.285657       1 apiapproval_controller.go:186] Starting KubernetesAPIApprovalPolicyConformantConditionController
I1130 16:16:21.285673       1 crd_finalizer.go:266] Starting CRDFinalizer
I1130 16:16:21.285734       1 controller.go:80] Starting OpenAPI V3 AggregationController
I1130 16:16:21.285801       1 crdregistration_controller.go:111] Starting crd-autoregister controller
I1130 16:16:21.285814       1 shared_informer.go:311] Waiting for caches to sync for crd-autoregister
I1130 16:16:21.285955       1 gc_controller.go:78] Starting apiserver lease garbage collector
I1130 16:16:21.285980       1 cluster_authentication_trust_controller.go:440] Starting cluster_authentication_trust_controller controller
I1130 16:16:21.285992       1 shared_informer.go:311] Waiting for caches to sync for cluster_authentication_trust_controller
I1130 16:16:21.286029       1 system_namespaces_controller.go:67] Starting system namespaces controller
I1130 16:16:21.286069       1 dynamic_cafile_content.go:157] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I1130 16:16:21.286125       1 controller.go:116] Starting legacy_token_tracking_controller
I1130 16:16:21.286137       1 shared_informer.go:311] Waiting for caches to sync for configmaps
I1130 16:16:21.286150       1 dynamic_cafile_content.go:157] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I1130 16:16:21.286495       1 gc_controller.go:78] Starting apiserver lease garbage collector
I1130 16:16:21.584061       1 cache.go:39] Caches are synced for AvailableConditionController controller
I1130 16:16:21.584819       1 apf_controller.go:377] Running API Priority and Fairness config worker
I1130 16:16:21.584830       1 apf_controller.go:380] Running API Priority and Fairness periodic rebalancing process
I1130 16:16:21.584833       1 cache.go:39] Caches are synced for APIServiceRegistrationController controller
I1130 16:16:21.585711       1 controller.go:624] quota admission added evaluator for: leases.coordination.k8s.io
I1130 16:16:21.586233       1 shared_informer.go:318] Caches are synced for cluster_authentication_trust_controller
I1130 16:16:21.586363       1 shared_informer.go:318] Caches are synced for configmaps
I1130 16:16:21.586411       1 shared_informer.go:318] Caches are synced for crd-autoregister
I1130 16:16:21.587640       1 aggregator.go:166] initial CRD sync complete...
I1130 16:16:21.587654       1 autoregister_controller.go:141] Starting autoregister controller
I1130 16:16:21.587670       1 cache.go:32] Waiting for caches to sync for autoregister controller
I1130 16:16:21.587679       1 cache.go:39] Caches are synced for autoregister controller
I1130 16:16:21.684123       1 shared_informer.go:318] Caches are synced for node_authorizer
E1130 16:16:21.697364       1 controller.go:97] Error removing old endpoints from kubernetes service: no API server IP addresses were listed in storage, refusing to erase all endpoints for the kubernetes Service
I1130 16:16:22.292793       1 storage_scheduling.go:111] all system priority classes are created successfully or already exist.
I1130 16:16:33.989510       1 controller.go:624] quota admission added evaluator for: endpoints
I1130 16:16:34.000053       1 controller.go:624] quota admission added evaluator for: endpointslices.discovery.k8s.io
I1130 16:17:55.916555       1 trace.go:236] Trace[1328875747]: "Get" accept:application/json, */*,audit-id:5e8ebabb-aaf8-42b2-b789-eb32ead75f75,client:192.168.49.2,protocol:HTTP/2.0,resource:endpoints,scope:resource,url:/api/v1/namespaces/kube-system/endpoints/k8s.io-minikube-hostpath,user-agent:storage-provisioner/v0.0.0 (linux/amd64) kubernetes/$Format,verb:GET (30-Nov-2023 16:17:54.686) (total time: 1229ms):
Trace[1328875747]: ---"About to write a response" 1229ms (16:17:55.915)
Trace[1328875747]: [1.229219122s] [1.229219122s] END
I1130 16:17:56.788969       1 trace.go:236] Trace[1957424443]: "Update" accept:application/json, */*,audit-id:8155bd6e-7aea-4883-bff7-5fc76eaabb5d,client:192.168.49.2,protocol:HTTP/2.0,resource:endpoints,scope:resource,url:/api/v1/namespaces/kube-system/endpoints/k8s.io-minikube-hostpath,user-agent:storage-provisioner/v0.0.0 (linux/amd64) kubernetes/$Format,verb:PUT (30-Nov-2023 16:17:55.918) (total time: 870ms):
Trace[1957424443]: ["GuaranteedUpdate etcd3" audit-id:8155bd6e-7aea-4883-bff7-5fc76eaabb5d,key:/services/endpoints/kube-system/k8s.io-minikube-hostpath,type:*core.Endpoints,resource:endpoints 870ms (16:17:55.918)
Trace[1957424443]:  ---"Txn call completed" 869ms (16:17:56.788)]
Trace[1957424443]: [870.382959ms] [870.382959ms] END
I1130 16:18:33.430617       1 trace.go:236] Trace[132123283]: "GuaranteedUpdate etcd3" audit-id:,key:/masterleases/192.168.49.2,type:*v1.Endpoints,resource:apiServerIPInfo (30-Nov-2023 16:18:32.713) (total time: 717ms):
Trace[132123283]: ---"Transaction prepared" 412ms (16:18:33.126)
Trace[132123283]: ---"Txn call completed" 303ms (16:18:33.430)
Trace[132123283]: [717.403836ms] [717.403836ms] END
I1130 16:32:37.872588       1 controller.go:624] quota admission added evaluator for: deployments.apps
I1130 16:32:37.886575       1 controller.go:624] quota admission added evaluator for: replicasets.apps
I1130 16:32:46.390450       1 alloc.go:330] "allocated clusterIPs" service="default/churnmodelling" clusterIPs={"IPv4":"10.106.42.84"}

* 
* ==> kube-controller-manager [85d0195b4002] <==
* I1128 15:15:35.638583       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/placement-app-57ffd56784" duration="246.63s"
I1128 15:15:50.636413       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/placement-app-57ffd56784" duration="148.452s"
I1128 15:18:23.765749       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/placement-app-57ffd56784" duration="17.093s"
I1128 15:19:14.447784       1 event.go:307] "Event occurred" object="default/placement-app" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set placement-app-5b5f96875 to 1"
I1128 15:19:14.459834       1 event.go:307] "Event occurred" object="default/placement-app-5b5f96875" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: placement-app-5b5f96875-t7xpn"
I1128 15:19:14.472127       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/placement-app-5b5f96875" duration="24.854152ms"
I1128 15:19:14.479653       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/placement-app-5b5f96875" duration="7.427461ms"
I1128 15:19:14.479978       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/placement-app-5b5f96875" duration="215.833s"
I1128 15:19:14.489360       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/placement-app-5b5f96875" duration="172.998s"
I1128 15:19:19.434981       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/placement-app-5b5f96875" duration="455.087s"
I1128 15:19:34.640551       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/placement-app-5b5f96875" duration="189.605s"
I1128 15:19:50.624044       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/placement-app-5b5f96875" duration="113.967s"
I1128 15:20:04.639768       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/placement-app-5b5f96875" duration="65.685s"
I1128 15:20:19.630914       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/placement-app-5b5f96875" duration="131.038s"
I1128 15:20:32.631219       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/placement-app-5b5f96875" duration="132s"
I1128 15:21:14.641434       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/placement-app-5b5f96875" duration="120.598s"
I1128 15:21:28.640612       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/placement-app-5b5f96875" duration="139.804s"
I1128 15:22:53.645541       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/placement-app-5b5f96875" duration="119.306s"
I1128 15:22:54.603655       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/placement-app-5b5f96875" duration="13.295s"
I1128 15:23:20.063639       1 event.go:307] "Event occurred" object="default/placement-app" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set placement-app-6498bdb476 to 1"
I1128 15:23:20.076554       1 event.go:307] "Event occurred" object="default/placement-app-6498bdb476" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: placement-app-6498bdb476-pfkjb"
I1128 15:23:20.087056       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/placement-app-6498bdb476" duration="24.148678ms"
I1128 15:23:20.093199       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/placement-app-6498bdb476" duration="6.021214ms"
I1128 15:23:20.093336       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/placement-app-6498bdb476" duration="49.684s"
I1128 15:23:20.099002       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/placement-app-6498bdb476" duration="118.404s"
I1128 15:25:58.230038       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/placement-app-6498bdb476" duration="9.041263ms"
I1128 15:25:58.230167       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/placement-app-6498bdb476" duration="47.481s"
I1128 15:26:34.979574       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/placement-app-6498bdb476" duration="17.684s"
I1128 15:27:23.289601       1 event.go:307] "Event occurred" object="default/placement-app" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set placement-app-6498bdb476 to 1"
I1128 15:27:23.303582       1 event.go:307] "Event occurred" object="default/placement-app-6498bdb476" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: placement-app-6498bdb476-89kch"
I1128 15:27:23.314388       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/placement-app-6498bdb476" duration="25.135023ms"
I1128 15:27:23.321865       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/placement-app-6498bdb476" duration="7.402493ms"
I1128 15:27:23.322009       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/placement-app-6498bdb476" duration="63.532s"
I1128 15:27:23.325156       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/placement-app-6498bdb476" duration="87.297s"
I1128 15:27:28.512134       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/placement-app-6498bdb476" duration="8.205444ms"
I1128 15:27:28.512239       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/placement-app-6498bdb476" duration="37.831s"
E1129 03:12:49.345335       1 resource_quota_controller.go:440] failed to discover resources: the server has asked for the client to provide credentials
I1129 03:12:49.432679       1 garbagecollector.go:818] "failed to discover preferred resources" error="the server has asked for the client to provide credentials"
E1129 05:52:21.424768       1 resource_quota_controller.go:440] failed to discover resources: the server has asked for the client to provide credentials
I1129 05:52:21.480644       1 garbagecollector.go:818] "failed to discover preferred resources" error="the server has asked for the client to provide credentials"
E1129 07:42:43.819964       1 resource_quota_controller.go:440] failed to discover resources: the server has asked for the client to provide credentials
I1129 07:42:43.819982       1 garbagecollector.go:818] "failed to discover preferred resources" error="the server has asked for the client to provide credentials"
E1129 12:32:35.617371       1 resource_quota_controller.go:440] failed to discover resources: the server has asked for the client to provide credentials
I1129 12:32:35.617397       1 garbagecollector.go:818] "failed to discover preferred resources" error="the server has asked for the client to provide credentials"
E1129 15:15:12.586211       1 resource_quota_controller.go:440] failed to discover resources: the server has asked for the client to provide credentials
I1129 15:15:12.586334       1 garbagecollector.go:818] "failed to discover preferred resources" error="the server has asked for the client to provide credentials"
I1129 19:50:19.404594       1 garbagecollector.go:818] "failed to discover preferred resources" error="the server has asked for the client to provide credentials"
E1129 19:50:19.404631       1 resource_quota_controller.go:440] failed to discover resources: the server has asked for the client to provide credentials
I1130 03:00:25.001214       1 garbagecollector.go:818] "failed to discover preferred resources" error="the server has asked for the client to provide credentials"
E1130 03:00:25.007405       1 resource_quota_controller.go:440] failed to discover resources: the server has asked for the client to provide credentials
E1130 06:24:09.022213       1 resource_quota_controller.go:440] failed to discover resources: the server has asked for the client to provide credentials
I1130 06:24:09.022283       1 garbagecollector.go:818] "failed to discover preferred resources" error="the server has asked for the client to provide credentials"
I1130 10:29:32.494920       1 garbagecollector.go:818] "failed to discover preferred resources" error="the server has asked for the client to provide credentials"
E1130 10:29:32.495251       1 resource_quota_controller.go:440] failed to discover resources: the server has asked for the client to provide credentials
E1130 13:10:14.110614       1 resource_quota_controller.go:440] failed to discover resources: the server has asked for the client to provide credentials
I1130 13:10:14.110654       1 garbagecollector.go:818] "failed to discover preferred resources" error="the server has asked for the client to provide credentials"
I1130 14:11:58.027276       1 garbagecollector.go:818] "failed to discover preferred resources" error="the server has asked for the client to provide credentials"
E1130 14:11:58.028133       1 resource_quota_controller.go:440] failed to discover resources: the server has asked for the client to provide credentials
I1130 16:12:18.478761       1 garbagecollector.go:818] "failed to discover preferred resources" error="the server has asked for the client to provide credentials"
E1130 16:12:18.478782       1 resource_quota_controller.go:440] failed to discover resources: the server has asked for the client to provide credentials

* 
* ==> kube-controller-manager [bb55d07969ca] <==
* I1130 16:16:33.882752       1 shared_informer.go:318] Caches are synced for node
I1130 16:16:33.882759       1 shared_informer.go:318] Caches are synced for PVC protection
I1130 16:16:33.882782       1 shared_informer.go:318] Caches are synced for certificate-csrsigning-legacy-unknown
I1130 16:16:33.882786       1 shared_informer.go:318] Caches are synced for persistent volume
I1130 16:16:33.882905       1 shared_informer.go:318] Caches are synced for endpoint_slice_mirroring
I1130 16:16:33.882941       1 shared_informer.go:318] Caches are synced for namespace
I1130 16:16:33.883148       1 shared_informer.go:318] Caches are synced for service account
I1130 16:16:33.883190       1 shared_informer.go:318] Caches are synced for bootstrap_signer
I1130 16:16:33.883312       1 range_allocator.go:174] "Sending events to api server"
I1130 16:16:33.883395       1 range_allocator.go:178] "Starting range CIDR allocator"
I1130 16:16:33.883407       1 shared_informer.go:311] Waiting for caches to sync for cidrallocator
I1130 16:16:33.883420       1 shared_informer.go:318] Caches are synced for cidrallocator
I1130 16:16:33.883607       1 shared_informer.go:318] Caches are synced for taint
I1130 16:16:33.883682       1 shared_informer.go:318] Caches are synced for ClusterRoleAggregator
I1130 16:16:33.883878       1 node_lifecycle_controller.go:1225] "Initializing eviction metric for zone" zone=""
I1130 16:16:33.884136       1 node_lifecycle_controller.go:877] "Missing timestamp for Node. Assuming now as a timestamp" node="minikube"
I1130 16:16:33.884227       1 taint_manager.go:206] "Starting NoExecuteTaintManager"
I1130 16:16:33.884292       1 taint_manager.go:211] "Sending events to api server"
I1130 16:16:33.884514       1 node_lifecycle_controller.go:1071] "Controller detected that zone is now in new state" zone="" newState="Normal"
I1130 16:16:33.884778       1 event.go:307] "Event occurred" object="minikube" fieldPath="" kind="Node" apiVersion="v1" type="Normal" reason="RegisteredNode" message="Node minikube event: Registered Node minikube in Controller"
I1130 16:16:33.884855       1 shared_informer.go:318] Caches are synced for certificate-csrsigning-kubelet-serving
I1130 16:16:33.884859       1 shared_informer.go:318] Caches are synced for endpoint
I1130 16:16:33.884919       1 shared_informer.go:318] Caches are synced for PV protection
I1130 16:16:33.884890       1 shared_informer.go:318] Caches are synced for crt configmap
I1130 16:16:33.887374       1 shared_informer.go:318] Caches are synced for certificate-csrsigning-kubelet-client
I1130 16:16:33.887603       1 shared_informer.go:318] Caches are synced for TTL
I1130 16:16:33.887736       1 shared_informer.go:318] Caches are synced for certificate-csrsigning-kube-apiserver-client
I1130 16:16:33.888756       1 shared_informer.go:318] Caches are synced for daemon sets
I1130 16:16:33.888975       1 shared_informer.go:318] Caches are synced for GC
I1130 16:16:33.890957       1 shared_informer.go:318] Caches are synced for disruption
I1130 16:16:33.893596       1 shared_informer.go:318] Caches are synced for deployment
I1130 16:16:33.897032       1 shared_informer.go:318] Caches are synced for ReplicaSet
I1130 16:16:33.897647       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/hello-minikube-7f54cff968" duration="468.974s"
I1130 16:16:33.897731       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/placement-app-6498bdb476" duration="43.974s"
I1130 16:16:33.897826       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-5dd5756b68" duration="62.029s"
I1130 16:16:33.897864       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kubernetes-dashboard/dashboard-metrics-scraper-7fd5cb4ddc" duration="77.629s"
I1130 16:16:33.898283       1 shared_informer.go:318] Caches are synced for stateful set
I1130 16:16:33.898389       1 shared_informer.go:311] Waiting for caches to sync for garbage collector
I1130 16:16:33.903690       1 shared_informer.go:318] Caches are synced for TTL after finished
I1130 16:16:33.983008       1 shared_informer.go:318] Caches are synced for endpoint_slice
I1130 16:16:33.983010       1 shared_informer.go:318] Caches are synced for HPA
I1130 16:16:33.987962       1 shared_informer.go:318] Caches are synced for job
I1130 16:16:34.009352       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-8694d4445c" duration="111.475892ms"
I1130 16:16:34.010521       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-8694d4445c" duration="80.725s"
I1130 16:16:34.082840       1 shared_informer.go:318] Caches are synced for attach detach
I1130 16:16:34.082845       1 shared_informer.go:318] Caches are synced for resource quota
I1130 16:16:34.093800       1 shared_informer.go:318] Caches are synced for cronjob
I1130 16:16:34.094797       1 shared_informer.go:318] Caches are synced for resource quota
I1130 16:16:34.399361       1 shared_informer.go:318] Caches are synced for garbage collector
I1130 16:16:34.439737       1 shared_informer.go:318] Caches are synced for garbage collector
I1130 16:16:34.439804       1 garbagecollector.go:166] "All resource monitors have synced. Proceeding to collect garbage"
I1130 16:16:37.085670       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-8694d4445c" duration="12.212486ms"
I1130 16:16:37.085956       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-8694d4445c" duration="99.801s"
I1130 16:32:37.890983       1 event.go:307] "Event occurred" object="default/churnmodelling" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set churnmodelling-55f7c66779 to 1"
I1130 16:32:37.906291       1 event.go:307] "Event occurred" object="default/churnmodelling-55f7c66779" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: churnmodelling-55f7c66779-w4xxz"
I1130 16:32:37.914417       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/churnmodelling-55f7c66779" duration="23.653881ms"
I1130 16:32:37.921972       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/churnmodelling-55f7c66779" duration="7.486967ms"
I1130 16:32:37.922120       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/churnmodelling-55f7c66779" duration="61.086s"
I1130 16:32:37.925835       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/churnmodelling-55f7c66779" duration="82.918s"
I1130 16:32:37.941800       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/churnmodelling-55f7c66779" duration="68.23s"

* 
* ==> kube-proxy [819d082ff381] <==
* I1128 14:55:50.817504       1 server_others.go:69] "Using iptables proxy"
I1128 14:55:50.833798       1 node.go:141] Successfully retrieved node IP: 192.168.49.2
I1128 14:55:50.865120       1 server.go:632] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I1128 14:55:50.867519       1 server_others.go:152] "Using iptables Proxier"
I1128 14:55:50.867606       1 server_others.go:421] "Detect-local-mode set to ClusterCIDR, but no cluster CIDR for family" ipFamily="IPv6"
I1128 14:55:50.867617       1 server_others.go:438] "Defaulting to no-op detect-local"
I1128 14:55:50.867758       1 proxier.go:251] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses"
I1128 14:55:50.868275       1 server.go:846] "Version info" version="v1.28.3"
I1128 14:55:50.868317       1 server.go:848] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I1128 14:55:50.869389       1 config.go:97] "Starting endpoint slice config controller"
I1128 14:55:50.869420       1 config.go:315] "Starting node config controller"
I1128 14:55:50.869448       1 shared_informer.go:311] Waiting for caches to sync for endpoint slice config
I1128 14:55:50.869465       1 shared_informer.go:311] Waiting for caches to sync for node config
I1128 14:55:50.869503       1 config.go:188] "Starting service config controller"
I1128 14:55:50.869510       1 shared_informer.go:311] Waiting for caches to sync for service config
I1128 14:55:50.969890       1 shared_informer.go:318] Caches are synced for service config
I1128 14:55:50.969972       1 shared_informer.go:318] Caches are synced for endpoint slice config
I1128 14:55:50.970053       1 shared_informer.go:318] Caches are synced for node config
I1129 12:40:02.409570       1 trace.go:236] Trace[352474388]: "iptables ChainExists" (29-Nov-2023 12:32:50.935) (total time: 6142ms):
Trace[352474388]: [6.142803016s] [6.142803016s] END

* 
* ==> kube-proxy [923e0d01ba3d] <==
* I1130 16:16:14.794311       1 server_others.go:69] "Using iptables proxy"
E1130 16:16:15.092445       1 node.go:130] Failed to retrieve node info: Get "https://control-plane.minikube.internal:8443/api/v1/nodes/minikube": dial tcp 192.168.49.2:8443: connect: connection refused
I1130 16:16:21.689676       1 node.go:141] Successfully retrieved node IP: 192.168.49.2
I1130 16:16:21.821268       1 server.go:632] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I1130 16:16:21.884500       1 server_others.go:152] "Using iptables Proxier"
I1130 16:16:21.884597       1 server_others.go:421] "Detect-local-mode set to ClusterCIDR, but no cluster CIDR for family" ipFamily="IPv6"
I1130 16:16:21.884610       1 server_others.go:438] "Defaulting to no-op detect-local"
I1130 16:16:21.885365       1 proxier.go:251] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses"
I1130 16:16:21.887035       1 server.go:846] "Version info" version="v1.28.3"
I1130 16:16:21.887094       1 server.go:848] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I1130 16:16:21.894364       1 config.go:97] "Starting endpoint slice config controller"
I1130 16:16:21.894426       1 config.go:188] "Starting service config controller"
I1130 16:16:21.894556       1 config.go:315] "Starting node config controller"
I1130 16:16:21.896363       1 shared_informer.go:311] Waiting for caches to sync for node config
I1130 16:16:21.896364       1 shared_informer.go:311] Waiting for caches to sync for service config
I1130 16:16:21.896395       1 shared_informer.go:311] Waiting for caches to sync for endpoint slice config
I1130 16:16:21.996637       1 shared_informer.go:318] Caches are synced for node config
I1130 16:16:21.996699       1 shared_informer.go:318] Caches are synced for service config
I1130 16:16:21.996829       1 shared_informer.go:318] Caches are synced for endpoint slice config

* 
* ==> kube-scheduler [3f72e2e31d33] <==
* I1130 16:16:15.803229       1 serving.go:348] Generated self-signed cert in-memory
W1130 16:16:21.395203       1 requestheader_controller.go:193] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W1130 16:16:21.395321       1 authentication.go:368] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W1130 16:16:21.395338       1 authentication.go:369] Continuing without authentication configuration. This may treat all requests as anonymous.
W1130 16:16:21.395351       1 authentication.go:370] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I1130 16:16:21.685804       1 server.go:154] "Starting Kubernetes Scheduler" version="v1.28.3"
I1130 16:16:21.685880       1 server.go:156] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I1130 16:16:21.696276       1 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I1130 16:16:21.697026       1 shared_informer.go:311] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I1130 16:16:21.697113       1 secure_serving.go:213] Serving securely on 127.0.0.1:10259
I1130 16:16:21.698757       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
I1130 16:16:21.797704       1 shared_informer.go:318] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file

* 
* ==> kube-scheduler [c04db572bd27] <==
* I1128 14:55:47.282646       1 serving.go:348] Generated self-signed cert in-memory
W1128 14:55:49.392384       1 requestheader_controller.go:193] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W1128 14:55:49.392512       1 authentication.go:368] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system": RBAC: [role.rbac.authorization.k8s.io "extension-apiserver-authentication-reader" not found, role.rbac.authorization.k8s.io "system::leader-locking-kube-scheduler" not found]
W1128 14:55:49.392577       1 authentication.go:369] Continuing without authentication configuration. This may treat all requests as anonymous.
W1128 14:55:49.392656       1 authentication.go:370] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I1128 14:55:49.497812       1 server.go:154] "Starting Kubernetes Scheduler" version="v1.28.3"
I1128 14:55:49.497876       1 server.go:156] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I1128 14:55:49.500474       1 secure_serving.go:213] Serving securely on 127.0.0.1:10259
I1128 14:55:49.500717       1 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I1128 14:55:49.500791       1 shared_informer.go:311] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I1128 14:55:49.500852       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
I1128 14:55:49.601306       1 shared_informer.go:318] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I1130 16:15:48.797514       1 tlsconfig.go:255] "Shutting down DynamicServingCertificateController"
I1130 16:15:48.893940       1 configmap_cafile_content.go:223] "Shutting down controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I1130 16:15:48.899778       1 secure_serving.go:258] Stopped listening on 127.0.0.1:10259
E1130 16:15:48.904534       1 run.go:74] "command failed" err="finished without leader elect"

* 
* ==> kubelet <==
* Nov 30 16:16:14 minikube kubelet[1986]: I1130 16:16:14.790420    1986 status_manager.go:853] "Failed to get status for pod" podUID="38298a1c-0f0a-4f6b-842a-b38f6edb0568" pod="kubernetes-dashboard/kubernetes-dashboard-8694d4445c-gzzhn" err="Get \"https://control-plane.minikube.internal:8443/api/v1/namespaces/kubernetes-dashboard/pods/kubernetes-dashboard-8694d4445c-gzzhn\": dial tcp 192.168.49.2:8443: connect: connection refused"
Nov 30 16:16:14 minikube kubelet[1986]: I1130 16:16:14.790973    1986 status_manager.go:853] "Failed to get status for pod" podUID="e3a9a342-3ae1-4bc1-abee-229d85d2d626" pod="default/hello-minikube-7f54cff968-9cwt9" err="Get \"https://control-plane.minikube.internal:8443/api/v1/namespaces/default/pods/hello-minikube-7f54cff968-9cwt9\": dial tcp 192.168.49.2:8443: connect: connection refused"
Nov 30 16:16:14 minikube kubelet[1986]: I1130 16:16:14.791417    1986 status_manager.go:853] "Failed to get status for pod" podUID="fa7ace73-3dbe-4a2e-9703-27141751fd5a" pod="default/placement-app-6498bdb476-89kch" err="Get \"https://control-plane.minikube.internal:8443/api/v1/namespaces/default/pods/placement-app-6498bdb476-89kch\": dial tcp 192.168.49.2:8443: connect: connection refused"
Nov 30 16:16:14 minikube kubelet[1986]: I1130 16:16:14.791815    1986 status_manager.go:853] "Failed to get status for pod" podUID="7da72fc2e2cfb27aacf6cffd1c72da00" pod="kube-system/kube-controller-manager-minikube" err="Get \"https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods/kube-controller-manager-minikube\": dial tcp 192.168.49.2:8443: connect: connection refused"
Nov 30 16:16:14 minikube kubelet[1986]: I1130 16:16:14.792142    1986 status_manager.go:853] "Failed to get status for pod" podUID="eaaa385f-aa8a-401a-87df-de07c73ca2a1" pod="kubernetes-dashboard/dashboard-metrics-scraper-7fd5cb4ddc-49kzf" err="Get \"https://control-plane.minikube.internal:8443/api/v1/namespaces/kubernetes-dashboard/pods/dashboard-metrics-scraper-7fd5cb4ddc-49kzf\": dial tcp 192.168.49.2:8443: connect: connection refused"
Nov 30 16:16:14 minikube kubelet[1986]: I1130 16:16:14.894267    1986 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="29148b03610064977b1d21a90b3e52396b71d90e201c059afe009318cbb27124"
Nov 30 16:16:15 minikube kubelet[1986]: I1130 16:16:15.187647    1986 scope.go:117] "RemoveContainer" containerID="f7fdc317b6a44ccccce157c99d71078a23d58ad4e8722adf173abb89c08bf9a4"
Nov 30 16:16:15 minikube kubelet[1986]: I1130 16:16:15.188261    1986 status_manager.go:853] "Failed to get status for pod" podUID="9aac5b5c8815def09a2ef9e37b89da55" pod="kube-system/etcd-minikube" err="Get \"https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods/etcd-minikube\": dial tcp 192.168.49.2:8443: connect: connection refused"
Nov 30 16:16:15 minikube kubelet[1986]: E1130 16:16:15.188318    1986 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"kubernetes-dashboard\" with CrashLoopBackOff: \"back-off 10s restarting failed container=kubernetes-dashboard pod=kubernetes-dashboard-8694d4445c-gzzhn_kubernetes-dashboard(38298a1c-0f0a-4f6b-842a-b38f6edb0568)\"" pod="kubernetes-dashboard/kubernetes-dashboard-8694d4445c-gzzhn" podUID="38298a1c-0f0a-4f6b-842a-b38f6edb0568"
Nov 30 16:16:15 minikube kubelet[1986]: I1130 16:16:15.188653    1986 status_manager.go:853] "Failed to get status for pod" podUID="9d33a503-5e3a-4089-ac3f-9a057e228d8f" pod="kube-system/storage-provisioner" err="Get \"https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods/storage-provisioner\": dial tcp 192.168.49.2:8443: connect: connection refused"
Nov 30 16:16:15 minikube kubelet[1986]: I1130 16:16:15.189029    1986 status_manager.go:853] "Failed to get status for pod" podUID="5fe9904a-ab92-4eb4-a075-dd97140d5780" pod="kube-system/kube-proxy-nhk66" err="Get \"https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods/kube-proxy-nhk66\": dial tcp 192.168.49.2:8443: connect: connection refused"
Nov 30 16:16:15 minikube kubelet[1986]: I1130 16:16:15.189404    1986 status_manager.go:853] "Failed to get status for pod" podUID="38298a1c-0f0a-4f6b-842a-b38f6edb0568" pod="kubernetes-dashboard/kubernetes-dashboard-8694d4445c-gzzhn" err="Get \"https://control-plane.minikube.internal:8443/api/v1/namespaces/kubernetes-dashboard/pods/kubernetes-dashboard-8694d4445c-gzzhn\": dial tcp 192.168.49.2:8443: connect: connection refused"
Nov 30 16:16:15 minikube kubelet[1986]: I1130 16:16:15.189848    1986 status_manager.go:853] "Failed to get status for pod" podUID="e3a9a342-3ae1-4bc1-abee-229d85d2d626" pod="default/hello-minikube-7f54cff968-9cwt9" err="Get \"https://control-plane.minikube.internal:8443/api/v1/namespaces/default/pods/hello-minikube-7f54cff968-9cwt9\": dial tcp 192.168.49.2:8443: connect: connection refused"
Nov 30 16:16:15 minikube kubelet[1986]: I1130 16:16:15.190189    1986 status_manager.go:853] "Failed to get status for pod" podUID="49002ca4-37e9-4f8d-a1ad-07297f94a5fa" pod="kube-system/coredns-5dd5756b68-mkpn4" err="Get \"https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods/coredns-5dd5756b68-mkpn4\": dial tcp 192.168.49.2:8443: connect: connection refused"
Nov 30 16:16:15 minikube kubelet[1986]: I1130 16:16:15.190526    1986 status_manager.go:853] "Failed to get status for pod" podUID="75ac196d3709dde303d8a81c035c2c28" pod="kube-system/kube-scheduler-minikube" err="Get \"https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods/kube-scheduler-minikube\": dial tcp 192.168.49.2:8443: connect: connection refused"
Nov 30 16:16:15 minikube kubelet[1986]: I1130 16:16:15.190992    1986 status_manager.go:853] "Failed to get status for pod" podUID="55b4bbe24dac3803a7379f9ae169d6ba" pod="kube-system/kube-apiserver-minikube" err="Get \"https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods/kube-apiserver-minikube\": dial tcp 192.168.49.2:8443: connect: connection refused"
Nov 30 16:16:15 minikube kubelet[1986]: I1130 16:16:15.191457    1986 status_manager.go:853] "Failed to get status for pod" podUID="fa7ace73-3dbe-4a2e-9703-27141751fd5a" pod="default/placement-app-6498bdb476-89kch" err="Get \"https://control-plane.minikube.internal:8443/api/v1/namespaces/default/pods/placement-app-6498bdb476-89kch\": dial tcp 192.168.49.2:8443: connect: connection refused"
Nov 30 16:16:15 minikube kubelet[1986]: I1130 16:16:15.191833    1986 status_manager.go:853] "Failed to get status for pod" podUID="eaaa385f-aa8a-401a-87df-de07c73ca2a1" pod="kubernetes-dashboard/dashboard-metrics-scraper-7fd5cb4ddc-49kzf" err="Get \"https://control-plane.minikube.internal:8443/api/v1/namespaces/kubernetes-dashboard/pods/dashboard-metrics-scraper-7fd5cb4ddc-49kzf\": dial tcp 192.168.49.2:8443: connect: connection refused"
Nov 30 16:16:15 minikube kubelet[1986]: I1130 16:16:15.192272    1986 status_manager.go:853] "Failed to get status for pod" podUID="7da72fc2e2cfb27aacf6cffd1c72da00" pod="kube-system/kube-controller-manager-minikube" err="Get \"https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods/kube-controller-manager-minikube\": dial tcp 192.168.49.2:8443: connect: connection refused"
Nov 30 16:16:15 minikube kubelet[1986]: I1130 16:16:15.295559    1986 status_manager.go:853] "Failed to get status for pod" podUID="9d33a503-5e3a-4089-ac3f-9a057e228d8f" pod="kube-system/storage-provisioner" err="Get \"https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods/storage-provisioner\": dial tcp 192.168.49.2:8443: connect: connection refused"
Nov 30 16:16:15 minikube kubelet[1986]: I1130 16:16:15.296043    1986 status_manager.go:853] "Failed to get status for pod" podUID="5fe9904a-ab92-4eb4-a075-dd97140d5780" pod="kube-system/kube-proxy-nhk66" err="Get \"https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods/kube-proxy-nhk66\": dial tcp 192.168.49.2:8443: connect: connection refused"
Nov 30 16:16:15 minikube kubelet[1986]: I1130 16:16:15.296532    1986 status_manager.go:853] "Failed to get status for pod" podUID="38298a1c-0f0a-4f6b-842a-b38f6edb0568" pod="kubernetes-dashboard/kubernetes-dashboard-8694d4445c-gzzhn" err="Get \"https://control-plane.minikube.internal:8443/api/v1/namespaces/kubernetes-dashboard/pods/kubernetes-dashboard-8694d4445c-gzzhn\": dial tcp 192.168.49.2:8443: connect: connection refused"
Nov 30 16:16:15 minikube kubelet[1986]: I1130 16:16:15.296978    1986 status_manager.go:853] "Failed to get status for pod" podUID="e3a9a342-3ae1-4bc1-abee-229d85d2d626" pod="default/hello-minikube-7f54cff968-9cwt9" err="Get \"https://control-plane.minikube.internal:8443/api/v1/namespaces/default/pods/hello-minikube-7f54cff968-9cwt9\": dial tcp 192.168.49.2:8443: connect: connection refused"
Nov 30 16:16:15 minikube kubelet[1986]: I1130 16:16:15.297446    1986 status_manager.go:853] "Failed to get status for pod" podUID="49002ca4-37e9-4f8d-a1ad-07297f94a5fa" pod="kube-system/coredns-5dd5756b68-mkpn4" err="Get \"https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods/coredns-5dd5756b68-mkpn4\": dial tcp 192.168.49.2:8443: connect: connection refused"
Nov 30 16:16:15 minikube kubelet[1986]: I1130 16:16:15.297756    1986 status_manager.go:853] "Failed to get status for pod" podUID="75ac196d3709dde303d8a81c035c2c28" pod="kube-system/kube-scheduler-minikube" err="Get \"https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods/kube-scheduler-minikube\": dial tcp 192.168.49.2:8443: connect: connection refused"
Nov 30 16:16:15 minikube kubelet[1986]: I1130 16:16:15.298029    1986 status_manager.go:853] "Failed to get status for pod" podUID="55b4bbe24dac3803a7379f9ae169d6ba" pod="kube-system/kube-apiserver-minikube" err="Get \"https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods/kube-apiserver-minikube\": dial tcp 192.168.49.2:8443: connect: connection refused"
Nov 30 16:16:15 minikube kubelet[1986]: I1130 16:16:15.298463    1986 status_manager.go:853] "Failed to get status for pod" podUID="9aac5b5c8815def09a2ef9e37b89da55" pod="kube-system/etcd-minikube" err="Get \"https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods/etcd-minikube\": dial tcp 192.168.49.2:8443: connect: connection refused"
Nov 30 16:16:15 minikube kubelet[1986]: I1130 16:16:15.298771    1986 status_manager.go:853] "Failed to get status for pod" podUID="fa7ace73-3dbe-4a2e-9703-27141751fd5a" pod="default/placement-app-6498bdb476-89kch" err="Get \"https://control-plane.minikube.internal:8443/api/v1/namespaces/default/pods/placement-app-6498bdb476-89kch\": dial tcp 192.168.49.2:8443: connect: connection refused"
Nov 30 16:16:15 minikube kubelet[1986]: I1130 16:16:15.299030    1986 status_manager.go:853] "Failed to get status for pod" podUID="7da72fc2e2cfb27aacf6cffd1c72da00" pod="kube-system/kube-controller-manager-minikube" err="Get \"https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods/kube-controller-manager-minikube\": dial tcp 192.168.49.2:8443: connect: connection refused"
Nov 30 16:16:15 minikube kubelet[1986]: I1130 16:16:15.299245    1986 status_manager.go:853] "Failed to get status for pod" podUID="eaaa385f-aa8a-401a-87df-de07c73ca2a1" pod="kubernetes-dashboard/dashboard-metrics-scraper-7fd5cb4ddc-49kzf" err="Get \"https://control-plane.minikube.internal:8443/api/v1/namespaces/kubernetes-dashboard/pods/dashboard-metrics-scraper-7fd5cb4ddc-49kzf\": dial tcp 192.168.49.2:8443: connect: connection refused"
Nov 30 16:16:15 minikube kubelet[1986]: I1130 16:16:15.393984    1986 status_manager.go:853] "Failed to get status for pod" podUID="49002ca4-37e9-4f8d-a1ad-07297f94a5fa" pod="kube-system/coredns-5dd5756b68-mkpn4" err="Get \"https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods/coredns-5dd5756b68-mkpn4\": dial tcp 192.168.49.2:8443: connect: connection refused"
Nov 30 16:16:15 minikube kubelet[1986]: I1130 16:16:15.394515    1986 status_manager.go:853] "Failed to get status for pod" podUID="75ac196d3709dde303d8a81c035c2c28" pod="kube-system/kube-scheduler-minikube" err="Get \"https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods/kube-scheduler-minikube\": dial tcp 192.168.49.2:8443: connect: connection refused"
Nov 30 16:16:15 minikube kubelet[1986]: I1130 16:16:15.395003    1986 status_manager.go:853] "Failed to get status for pod" podUID="55b4bbe24dac3803a7379f9ae169d6ba" pod="kube-system/kube-apiserver-minikube" err="Get \"https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods/kube-apiserver-minikube\": dial tcp 192.168.49.2:8443: connect: connection refused"
Nov 30 16:16:15 minikube kubelet[1986]: I1130 16:16:15.395552    1986 status_manager.go:853] "Failed to get status for pod" podUID="9aac5b5c8815def09a2ef9e37b89da55" pod="kube-system/etcd-minikube" err="Get \"https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods/etcd-minikube\": dial tcp 192.168.49.2:8443: connect: connection refused"
Nov 30 16:16:15 minikube kubelet[1986]: I1130 16:16:15.395981    1986 status_manager.go:853] "Failed to get status for pod" podUID="9d33a503-5e3a-4089-ac3f-9a057e228d8f" pod="kube-system/storage-provisioner" err="Get \"https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods/storage-provisioner\": dial tcp 192.168.49.2:8443: connect: connection refused"
Nov 30 16:16:15 minikube kubelet[1986]: I1130 16:16:15.396360    1986 status_manager.go:853] "Failed to get status for pod" podUID="5fe9904a-ab92-4eb4-a075-dd97140d5780" pod="kube-system/kube-proxy-nhk66" err="Get \"https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods/kube-proxy-nhk66\": dial tcp 192.168.49.2:8443: connect: connection refused"
Nov 30 16:16:15 minikube kubelet[1986]: I1130 16:16:15.396758    1986 status_manager.go:853] "Failed to get status for pod" podUID="38298a1c-0f0a-4f6b-842a-b38f6edb0568" pod="kubernetes-dashboard/kubernetes-dashboard-8694d4445c-gzzhn" err="Get \"https://control-plane.minikube.internal:8443/api/v1/namespaces/kubernetes-dashboard/pods/kubernetes-dashboard-8694d4445c-gzzhn\": dial tcp 192.168.49.2:8443: connect: connection refused"
Nov 30 16:16:15 minikube kubelet[1986]: I1130 16:16:15.397196    1986 status_manager.go:853] "Failed to get status for pod" podUID="e3a9a342-3ae1-4bc1-abee-229d85d2d626" pod="default/hello-minikube-7f54cff968-9cwt9" err="Get \"https://control-plane.minikube.internal:8443/api/v1/namespaces/default/pods/hello-minikube-7f54cff968-9cwt9\": dial tcp 192.168.49.2:8443: connect: connection refused"
Nov 30 16:16:15 minikube kubelet[1986]: I1130 16:16:15.397631    1986 status_manager.go:853] "Failed to get status for pod" podUID="fa7ace73-3dbe-4a2e-9703-27141751fd5a" pod="default/placement-app-6498bdb476-89kch" err="Get \"https://control-plane.minikube.internal:8443/api/v1/namespaces/default/pods/placement-app-6498bdb476-89kch\": dial tcp 192.168.49.2:8443: connect: connection refused"
Nov 30 16:16:15 minikube kubelet[1986]: I1130 16:16:15.397982    1986 status_manager.go:853] "Failed to get status for pod" podUID="7da72fc2e2cfb27aacf6cffd1c72da00" pod="kube-system/kube-controller-manager-minikube" err="Get \"https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods/kube-controller-manager-minikube\": dial tcp 192.168.49.2:8443: connect: connection refused"
Nov 30 16:16:15 minikube kubelet[1986]: I1130 16:16:15.398397    1986 status_manager.go:853] "Failed to get status for pod" podUID="eaaa385f-aa8a-401a-87df-de07c73ca2a1" pod="kubernetes-dashboard/dashboard-metrics-scraper-7fd5cb4ddc-49kzf" err="Get \"https://control-plane.minikube.internal:8443/api/v1/namespaces/kubernetes-dashboard/pods/dashboard-metrics-scraper-7fd5cb4ddc-49kzf\": dial tcp 192.168.49.2:8443: connect: connection refused"
Nov 30 16:16:16 minikube kubelet[1986]: I1130 16:16:16.993471    1986 scope.go:117] "RemoveContainer" containerID="9500a604b9dfd3c879c0cbcbfefd9af9dcbdf8bd23df8773bb025e668d4f5b6d"
Nov 30 16:16:16 minikube kubelet[1986]: I1130 16:16:16.993942    1986 scope.go:117] "RemoveContainer" containerID="f7fdc317b6a44ccccce157c99d71078a23d58ad4e8722adf173abb89c08bf9a4"
Nov 30 16:16:16 minikube kubelet[1986]: E1130 16:16:16.994334    1986 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"kubernetes-dashboard\" with CrashLoopBackOff: \"back-off 10s restarting failed container=kubernetes-dashboard pod=kubernetes-dashboard-8694d4445c-gzzhn_kubernetes-dashboard(38298a1c-0f0a-4f6b-842a-b38f6edb0568)\"" pod="kubernetes-dashboard/kubernetes-dashboard-8694d4445c-gzzhn" podUID="38298a1c-0f0a-4f6b-842a-b38f6edb0568"
Nov 30 16:16:18 minikube kubelet[1986]: I1130 16:16:18.212644    1986 scope.go:117] "RemoveContainer" containerID="f7fdc317b6a44ccccce157c99d71078a23d58ad4e8722adf173abb89c08bf9a4"
Nov 30 16:16:18 minikube kubelet[1986]: E1130 16:16:18.213026    1986 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"kubernetes-dashboard\" with CrashLoopBackOff: \"back-off 10s restarting failed container=kubernetes-dashboard pod=kubernetes-dashboard-8694d4445c-gzzhn_kubernetes-dashboard(38298a1c-0f0a-4f6b-842a-b38f6edb0568)\"" pod="kubernetes-dashboard/kubernetes-dashboard-8694d4445c-gzzhn" podUID="38298a1c-0f0a-4f6b-842a-b38f6edb0568"
Nov 30 16:16:21 minikube kubelet[1986]: E1130 16:16:21.486986    1986 reflector.go:147] object-"kubernetes-dashboard"/"kube-root-ca.crt": Failed to watch *v1.ConfigMap: unknown (get configmaps)
Nov 30 16:16:21 minikube kubelet[1986]: E1130 16:16:21.491445    1986 reflector.go:147] object-"default"/"kube-root-ca.crt": Failed to watch *v1.ConfigMap: unknown (get configmaps)
Nov 30 16:16:21 minikube kubelet[1986]: E1130 16:16:21.491622    1986 reflector.go:147] object-"kube-system"/"kube-root-ca.crt": Failed to watch *v1.ConfigMap: unknown (get configmaps)
Nov 30 16:16:23 minikube kubelet[1986]: I1130 16:16:23.402377    1986 scope.go:117] "RemoveContainer" containerID="f7fdc317b6a44ccccce157c99d71078a23d58ad4e8722adf173abb89c08bf9a4"
Nov 30 16:16:23 minikube kubelet[1986]: E1130 16:16:23.402910    1986 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"kubernetes-dashboard\" with CrashLoopBackOff: \"back-off 10s restarting failed container=kubernetes-dashboard pod=kubernetes-dashboard-8694d4445c-gzzhn_kubernetes-dashboard(38298a1c-0f0a-4f6b-842a-b38f6edb0568)\"" pod="kubernetes-dashboard/kubernetes-dashboard-8694d4445c-gzzhn" podUID="38298a1c-0f0a-4f6b-842a-b38f6edb0568"
Nov 30 16:16:30 minikube kubelet[1986]: I1130 16:16:30.099514    1986 scope.go:117] "RemoveContainer" containerID="17e825af3e70b7c1d1e801c09da44d138ca3cc4860f4884a84d36ba17078ba6e"
Nov 30 16:16:36 minikube kubelet[1986]: I1130 16:16:36.098644    1986 scope.go:117] "RemoveContainer" containerID="f7fdc317b6a44ccccce157c99d71078a23d58ad4e8722adf173abb89c08bf9a4"
Nov 30 16:18:09 minikube kubelet[1986]: W1130 16:18:09.210374    1986 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Nov 30 16:23:09 minikube kubelet[1986]: W1130 16:23:09.214309    1986 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Nov 30 16:28:09 minikube kubelet[1986]: W1130 16:28:09.215486    1986 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Nov 30 16:32:37 minikube kubelet[1986]: I1130 16:32:37.927988    1986 topology_manager.go:215] "Topology Admit Handler" podUID="34928984-797b-4718-90ac-0ea6543d927d" podNamespace="default" podName="churnmodelling-55f7c66779-w4xxz"
Nov 30 16:32:38 minikube kubelet[1986]: I1130 16:32:38.100419    1986 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-47w7f\" (UniqueName: \"kubernetes.io/projected/34928984-797b-4718-90ac-0ea6543d927d-kube-api-access-47w7f\") pod \"churnmodelling-55f7c66779-w4xxz\" (UID: \"34928984-797b-4718-90ac-0ea6543d927d\") " pod="default/churnmodelling-55f7c66779-w4xxz"
Nov 30 16:32:38 minikube kubelet[1986]: I1130 16:32:38.701014    1986 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="4268e589068754a6c5a0047ff299cf5a49c5c66e7db530bf0fde88131307a7c0"
Nov 30 16:33:09 minikube kubelet[1986]: W1130 16:33:09.216196    1986 sysinfo.go:203] Nodes topology is not available, providing CPU topology

* 
* ==> kubernetes-dashboard [474d57da9ea5] <==
* 2023/11/30 16:16:36 Starting overwatch
2023/11/30 16:16:36 Using namespace: kubernetes-dashboard
2023/11/30 16:16:36 Using in-cluster config to connect to apiserver
2023/11/30 16:16:36 Using secret token for csrf signing
2023/11/30 16:16:36 Initializing csrf token from kubernetes-dashboard-csrf secret
2023/11/30 16:16:36 Empty token. Generating and storing in a secret kubernetes-dashboard-csrf
2023/11/30 16:16:36 Successful initial request to the apiserver, version: v1.28.3
2023/11/30 16:16:36 Generating JWE encryption key
2023/11/30 16:16:36 New synchronizer has been registered: kubernetes-dashboard-key-holder-kubernetes-dashboard. Starting
2023/11/30 16:16:36 Starting secret synchronizer for kubernetes-dashboard-key-holder in namespace kubernetes-dashboard
2023/11/30 16:16:36 Initializing JWE encryption key from synchronized object
2023/11/30 16:16:36 Creating in-cluster Sidecar client
2023/11/30 16:16:36 Successful request to sidecar
2023/11/30 16:16:36 Serving insecurely on HTTP port: 9090

* 
* ==> kubernetes-dashboard [f7fdc317b6a4] <==
* 2023/11/30 16:16:14 Starting overwatch
panic: Get "https://10.96.0.1:443/api/v1/namespaces/kubernetes-dashboard/secrets/kubernetes-dashboard-csrf": dial tcp 10.96.0.1:443: connect: connection refused

goroutine 1 [running]:
github.com/kubernetes/dashboard/src/app/backend/client/csrf.(*csrfTokenManager).init(0xc00069fae8)
	/home/runner/work/dashboard/dashboard/src/app/backend/client/csrf/manager.go:41 +0x30e
github.com/kubernetes/dashboard/src/app/backend/client/csrf.NewCsrfTokenManager(...)
	/home/runner/work/dashboard/dashboard/src/app/backend/client/csrf/manager.go:66
github.com/kubernetes/dashboard/src/app/backend/client.(*clientManager).initCSRFKey(0xc00047a100)
	/home/runner/work/dashboard/dashboard/src/app/backend/client/manager.go:527 +0x94
github.com/kubernetes/dashboard/src/app/backend/client.(*clientManager).init(0x19aba3a?)
	/home/runner/work/dashboard/dashboard/src/app/backend/client/manager.go:495 +0x32
github.com/kubernetes/dashboard/src/app/backend/client.NewClientManager(...)
	/home/runner/work/dashboard/dashboard/src/app/backend/client/manager.go:594
main.main()
	/home/runner/work/dashboard/dashboard/src/app/backend/dashboard.go:96 +0x1cf
2023/11/30 16:16:14 Using namespace: kubernetes-dashboard
2023/11/30 16:16:14 Using in-cluster config to connect to apiserver
2023/11/30 16:16:14 Using secret token for csrf signing
2023/11/30 16:16:14 Initializing csrf token from kubernetes-dashboard-csrf secret

* 
* ==> storage-provisioner [17e825af3e70] <==
* I1130 16:16:11.595559       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
F1130 16:16:11.985551       1 main.go:39] error getting server version: Get "https://10.96.0.1:443/version?timeout=32s": dial tcp 10.96.0.1:443: connect: connection refused

* 
* ==> storage-provisioner [95ed8f741496] <==
* I1130 16:16:30.338601       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
I1130 16:16:30.366849       1 storage_provisioner.go:141] Storage provisioner initialized, now starting service!
I1130 16:16:30.367203       1 leaderelection.go:243] attempting to acquire leader lease kube-system/k8s.io-minikube-hostpath...
I1130 16:16:47.847605       1 leaderelection.go:253] successfully acquired lease kube-system/k8s.io-minikube-hostpath
I1130 16:16:47.848265       1 controller.go:835] Starting provisioner controller k8s.io/minikube-hostpath_minikube_e806b669-3d2b-47ae-80e9-ec7895e19d79!
I1130 16:16:47.848309       1 event.go:282] Event(v1.ObjectReference{Kind:"Endpoints", Namespace:"kube-system", Name:"k8s.io-minikube-hostpath", UID:"6ff7860a-77ad-4696-9ba4-94353026d48e", APIVersion:"v1", ResourceVersion:"42777", FieldPath:""}): type: 'Normal' reason: 'LeaderElection' minikube_e806b669-3d2b-47ae-80e9-ec7895e19d79 became leader
I1130 16:16:47.983424       1 controller.go:884] Started provisioner controller k8s.io/minikube-hostpath_minikube_e806b669-3d2b-47ae-80e9-ec7895e19d79!

